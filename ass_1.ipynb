{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "# MACHINE_1_P = \"./data/set2_machine.json\"\n",
    "# HUMAN_1_P = \"./data/set2_human.json\"\n",
    "MACHINE_1_P = \"./data/set1_machine.json\"\n",
    "HUMAN_1_P = \"./data/set1_human.json\"\n",
    "MACHINE_2_P = \"./data/set2_machine.json\"\n",
    "HUMAN_2_P = \"./data/set2_human.json\"\n",
    "MACHINE_IND = 1\n",
    "HUMAND_IND = 0\n",
    "\n",
    "TEST_FRA = 0.3\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "def sparse_transf(info_li, vectorizer, fit=True):\n",
    "    data_dicts = []\n",
    "    for record in info_li:\n",
    "        word_dict = defaultdict(int)\n",
    "        for token in record:\n",
    "            word_dict[token] += 1\n",
    "        data_dicts.append(word_dict)\n",
    "    if fit == True:\n",
    "        return vectorizer.fit_transform(data_dicts)\n",
    "    return vectorizer.transform(data_dicts)\n",
    "\n",
    "\n",
    "prompt_1 = []\n",
    "text_1 = []\n",
    "label_1 = []\n",
    "# read machine_set_1 data\n",
    "with open(MACHINE_1_P, 'r') as file:\n",
    "    \n",
    "    f_data = json.load(file)\n",
    "    prompt_1 += [i[\"prompt\"] for i in f_data]\n",
    "    text_1 += [i[\"txt\"] for i in f_data]\n",
    "    label_1 += [MACHINE_IND for i in range(len(f_data))]\n",
    "\n",
    "# read human_set_1 data\n",
    "with open(HUMAN_1_P, 'r') as file:\n",
    "    f_data = json.load(file)\n",
    "    prompt_1 += [i[\"prompt\"] for i in f_data]\n",
    "    text_1 += [i[\"txt\"] for i in f_data]\n",
    "    label_1 += [HUMAND_IND for i in range(len(f_data))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing - shuffle / train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4900\n",
      "2450\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.utils import resample\n",
    "N_SAMPLE = 100\n",
    "\n",
    "\n",
    "# # StratifiedShuffleSplit\n",
    "# sss = StratifiedShuffleSplit(n_splits=1, test_size=N_SAMPLE, random_state=42)\n",
    "# for train_index, test_index in sss.split(data_1, label_1):\n",
    "#     data_1_train, data_1_test = [data_1[i] for i in train_index], [data_1[i] for i in test_index]\n",
    "#     label_1_train, label_1_test = [label_1[i] for i in train_index], [label_1[i] for i in test_index]\n",
    "# print(len(label_1_train), sum(label_1_train))\n",
    "\n",
    "# _______________ train_test_split _______________\n",
    "data_1 = list(zip(prompt_1, text_1))\n",
    "data_1_train, data_1_test, label_1_train, label_1_test = train_test_split(data_1, label_1, test_size=TEST_FRA, stratify=label_1, random_state=RANDOM_SEED)\n",
    "\n",
    "\n",
    "# _______________ Resampling / sub-sampling _______________\n",
    "h_1 = []\n",
    "m_1 = []\n",
    "for i in range(len(data_1_train)):\n",
    "    if label_1_train[i] == MACHINE_IND:\n",
    "        m_1.append(data_1_train[i])\n",
    "    else:\n",
    "        h_1.append(data_1_train[i])\n",
    "n_samples = min(len(h_1), len(m_1))\n",
    "\n",
    "h_1 = resample(h_1, n_samples = n_samples, random_state=0)\n",
    "m_1 = resample(m_1, n_samples = n_samples, random_state=0)\n",
    "data_1_train = h_1+m_1\n",
    "label_1_train = [HUMAND_IND for i in range(n_samples)]+[MACHINE_IND for i in range(n_samples)]\n",
    "print(len(data_1_train))\n",
    "print(sum(label_1_train))\n",
    "data_1_train, d_t_1, label_1_train, l_t_1 = train_test_split(data_1_train, label_1_train, test_size=TEST_FRA, random_state=RANDOM_SEED)\n",
    "data_1_train += d_t_1\n",
    "label_1_train += l_t_1\n",
    "\n",
    "\n",
    "\n",
    "# unzip\n",
    "prompt_1_train, text_1_train = zip(*data_1_train)\n",
    "prompt_1_train, text_1_train = list(prompt_1_train), list(text_1_train)\n",
    "\n",
    "prompt_1_test, text_1_test = zip(*data_1_test)\n",
    "prompt_1_test, text_1_test = list(prompt_1_test), list(text_1_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark -- Naive Bayes / Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes' on prompt:\n",
      "4900\n",
      "2450\n",
      "Accuracy Score:  0.7542857142857143\n",
      "Marco F1 score:  0.7539754836210791\n",
      "Precision:  0.7555748076428479\n",
      "Recall:  0.7542857142857142\n",
      "Confusion matrix: \n",
      "[[1761  689]\n",
      " [ 515 1935]]\n",
      "\n",
      "Logistic Regression on prompt: \n",
      "4900\n",
      "2450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junhual1/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/junhual1/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/junhual1/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/junhual1/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/junhual1/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.8295918367346938\n",
      "Marco F1 score:  0.828477224445821\n",
      "Precision:  0.8383876609409944\n",
      "Recall:  0.8295918367346939\n",
      "Confusion matrix: \n",
      "[[1835  615]\n",
      " [ 220 2230]]\n",
      "\n",
      "Naive Bayes' on text:\n",
      "4900\n",
      "2450\n",
      "Accuracy Score:  0.7687755102040816\n",
      "Marco F1 score:  0.7664495284335089\n",
      "Precision:  0.7799269317530566\n",
      "Recall:  0.7687755102040816\n",
      "Confusion matrix: \n",
      "[[2128  322]\n",
      " [ 811 1639]]\n",
      "\n",
      "Logistic Regression on text: \n",
      "4900\n",
      "2450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junhual1/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/junhual1/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/junhual1/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/junhual1/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.9175510204081633\n",
      "Marco F1 score:  0.9175124185562225\n",
      "Precision:  0.9183340944415068\n",
      "Recall:  0.9175510204081633\n",
      "Confusion matrix: \n",
      "[[2195  255]\n",
      " [ 149 2301]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junhual1/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "CV_NUM = 5\n",
    "\n",
    "def vali_model(clf, d_train, label_train, cv_num = CV_NUM):\n",
    "    print(len(label_train))\n",
    "    print(sum(label_train))\n",
    "    label_pred = cross_val_predict(clf, d_train, label_train, cv=cv_num)\n",
    "    acc_score = accuracy_score(label_train, label_pred)\n",
    "    print(\"Accuracy Score: \", acc_score)\n",
    "    print(\"Marco F1 score: \", f1_score(label_train, label_pred, average=\"macro\"))\n",
    "    print(\"Precision: \", precision_score(label_train, label_pred, average=\"macro\"))\n",
    "    print(\"Recall: \", recall_score(label_train, label_pred, average=\"macro\"))\n",
    "    print(\"Confusion matrix: \")\n",
    "    print(confusion_matrix(label_train, label_pred))\n",
    "    print()\n",
    "\n",
    "\n",
    "# _______________ vectorize _______________\n",
    "p_dv = DictVectorizer()\n",
    "t_dv = DictVectorizer()\n",
    "prompt_1_train_v = sparse_transf(prompt_1_train, p_dv)\n",
    "prompt_1_test_v = sparse_transf(prompt_1_test, p_dv, False)\n",
    "text_1_train_v = sparse_transf(text_1_train, t_dv)\n",
    "text_1_test_v = sparse_transf(text_1_test, t_dv, False)\n",
    "\n",
    "# _______________ Promp _______________\n",
    "# Naive bayes on promp\n",
    "print(\"Naive Bayes' on prompt:\")\n",
    "p_nb_clf = MultinomialNB()\n",
    "vali_model(p_nb_clf, prompt_1_train_v, label_1_train)\n",
    "# Logistic Regression on promp\n",
    "print(\"Logistic Regression on prompt: \")\n",
    "p_lr_clf = LogisticRegression()\n",
    "vali_model(p_lr_clf, prompt_1_train_v, label_1_train)\n",
    "\n",
    "# _______________ text _______________\n",
    "# Naive bayes on text\n",
    "print(\"Naive Bayes' on text:\")\n",
    "t_nb_clf = MultinomialNB()\n",
    "vali_model(t_nb_clf, text_1_train_v, label_1_train)\n",
    "# Logistic Regression on text\n",
    "print(\"Logistic Regression on text: \")\n",
    "t_lr_clf = LogisticRegression()\n",
    "vali_model(t_lr_clf, text_1_train_v, label_1_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "NUM_TOKEN = 5000\n",
    "MAX_PRO_LEN = 50\n",
    "MAX_TEXT_LEN = 200\n",
    "NO_EPO = 4\n",
    "NO_BAT = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-09 13:04:04.269798: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-04-09 13:04:06.881071: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "490/490 [==============================] - 36s 60ms/step - loss: 0.5382 - accuracy: 0.7167\n",
      "Epoch 2/5\n",
      "490/490 [==============================] - 29s 59ms/step - loss: 0.1739 - accuracy: 0.9451\n",
      "Epoch 3/5\n",
      "490/490 [==============================] - 29s 60ms/step - loss: 0.0838 - accuracy: 0.9773\n",
      "Epoch 4/5\n",
      "490/490 [==============================] - 28s 58ms/step - loss: 0.0568 - accuracy: 0.9855\n",
      "Epoch 5/5\n",
      "490/490 [==============================] - 29s 60ms/step - loss: 0.0462 - accuracy: 0.9884\n",
      "1183/1183 [==============================] - 28s 22ms/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'confusion_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[31], line 25\u001b[0m\n",
      "\u001b[1;32m     22\u001b[0m loss, accuracy \u001b[39m=\u001b[39m p_model_rnn\u001b[39m.\u001b[39mevaluate(prompt_1_test_np, label_1_test_np, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32m     24\u001b[0m prompt_1_pre_rnn \u001b[39m=\u001b[39m p_model_rnn\u001b[39m.\u001b[39mpredict(prompt_1_test_np)\n",
      "\u001b[0;32m---> 25\u001b[0m confusion \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mconfusion_matrix(labels\u001b[39m=\u001b[39mlabel_1_test_np, predictions\u001b[39m=\u001b[39mprompt_1_pre_rnn, num_classes\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "\u001b[1;32m     26\u001b[0m \u001b[39mprint\u001b[39m(confusion)\n",
      "\u001b[1;32m     28\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mloss: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, accuracy: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(loss, accuracy))\n",
      "\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'confusion_matrix'"
     ]
    }
   ],
   "source": [
    "## _______________ RNN on Prompts _______________\n",
    "\n",
    "p_model_rnn= tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=NUM_TOKEN,\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "p_model_rnn.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "prompt_1_train_np = pad_sequences(prompt_1_train, padding='post', maxlen=MAX_PRO_LEN)\n",
    "label_1_train_np = np.array(label_1_train)\n",
    "\n",
    "p_model_rnn.fit(prompt_1_train_np, label_1_train_np, epochs=NO_EPO, verbose=True, batch_size=NO_BAT)\n",
    "\n",
    "prompt_1_test_np = pad_sequences(prompt_1_test, padding='post', maxlen=MAX_PRO_LEN)\n",
    "label_1_test_np = np.array(label_1_test)\n",
    "loss, accuracy = p_model_rnn.evaluate(prompt_1_test_np, label_1_test_np, verbose=False)\n",
    "\n",
    "prompt_1_pre_rnn = p_model_rnn.predict(prompt_1_test_np)\n",
    "confusion = tf.math.confusion_matrix(labels=label_1_test_np, predictions=prompt_1_pre_rnn, num_classes=2)\n",
    "print(confusion)\n",
    "\n",
    "print(\"loss: {}, accuracy: {}\".format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-09 13:28:49.741935: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-04-09 13:28:51.846716: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "490/490 [==============================] - 190s 376ms/step - loss: 0.4794 - accuracy: 0.7588\n",
      "Epoch 2/4\n",
      "490/490 [==============================] - 171s 349ms/step - loss: 0.1869 - accuracy: 0.9288\n",
      "Epoch 3/4\n",
      "490/490 [==============================] - 189s 385ms/step - loss: 0.0570 - accuracy: 0.9814\n",
      "Epoch 4/4\n",
      "490/490 [==============================] - 161s 329ms/step - loss: 0.0221 - accuracy: 0.9945\n",
      "1183/1183 [==============================] - 40s 32ms/step\n",
      "tf.Tensor(\n",
      "[[36776     0]\n",
      " [ 1050     0]], shape=(2, 2), dtype=int32)\n",
      "loss: 2.1054835319519043, accuracy: 0.57317715883255\n"
     ]
    }
   ],
   "source": [
    "# _______________ RNN on Text _______________\n",
    "t_model_rnn= tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=NUM_TOKEN,\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "t_model_rnn.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "text_1_train_np = pad_sequences(text_1_train, padding='post', maxlen=MAX_TEXT_LEN)\n",
    "label_1_train_np = np.array(label_1_train)\n",
    "t_model_rnn.fit(text_1_train_np, label_1_train_np, epochs=NO_EPO, verbose=True, batch_size=NO_BAT)\n",
    "\n",
    "text_1_test_np = pad_sequences(text_1_train, padding='post', maxlen=MAX_TEXT_LEN)\n",
    "label_1_test_np = np.array(label_1_test)\n",
    "loss, accuracy = t_model_rnn.evaluate(prompt_1_test_np, label_1_test_np, verbose=False)\n",
    "\n",
    "text_1_pre_rnn = t_model_rnn.predict(prompt_1_test_np)\n",
    "confusion = tf.math.confusion_matrix(labels=label_1_test, predictions=text_1_pre_rnn, num_classes=2)\n",
    "print(confusion)\n",
    "\n",
    "print(\"loss: {}, accuracy: {}\".format(loss, accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
