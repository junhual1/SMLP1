{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "# MACHINE_1_P = \"./data/set2_machine.json\"\n",
    "# HUMAN_1_P = \"./data/set2_human.json\"\n",
    "MACHINE_1_P = \"./data/set1_machine.json\"\n",
    "HUMAN_1_P = \"./data/set1_human.json\"\n",
    "MACHINE_2_P = \"./data/set2_machine.json\"\n",
    "HUMAN_2_P = \"./data/set2_human.json\"\n",
    "MACHINE_IND = 1\n",
    "HUMAND_IND = 0\n",
    "\n",
    "TEST_FRA = 0.3\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "def sparse_transf(info_li, vectorizer, fit=True):\n",
    "    data_dicts = []\n",
    "    for record in info_li:\n",
    "        word_dict = defaultdict(int)\n",
    "        for token in record:\n",
    "            word_dict[token] += 1\n",
    "        data_dicts.append(word_dict)\n",
    "    if fit == True:\n",
    "        return vectorizer.fit_transform(data_dicts)\n",
    "    return vectorizer.transform(data_dicts)\n",
    "\n",
    "\n",
    "prompt_1 = []\n",
    "txt_1 = []\n",
    "label_1 = []\n",
    "# read machine_set_1 data\n",
    "with open(MACHINE_1_P, 'r') as file:\n",
    "    \n",
    "    f_data = json.load(file)\n",
    "    prompt_1 += [i[\"prompt\"] for i in f_data]\n",
    "    txt_1 += [i[\"txt\"] for i in f_data]\n",
    "    label_1 += [MACHINE_IND for i in range(len(f_data))]\n",
    "\n",
    "# read human_set_1 data\n",
    "with open(HUMAN_1_P, 'r') as file:\n",
    "    f_data = json.load(file)\n",
    "    prompt_1 += [i[\"prompt\"] for i in f_data]\n",
    "    txt_1 += [i[\"txt\"] for i in f_data]\n",
    "    label_1 += [HUMAND_IND for i in range(len(f_data))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4900\n",
      "2450\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.utils import resample\n",
    "N_SAMPLE = 100\n",
    "\n",
    "\n",
    "# # StratifiedShuffleSplit\n",
    "# sss = StratifiedShuffleSplit(n_splits=1, test_size=N_SAMPLE, random_state=42)\n",
    "# for train_index, test_index in sss.split(data_1, label_1):\n",
    "#     data_1_train, data_1_test = [data_1[i] for i in train_index], [data_1[i] for i in test_index]\n",
    "#     label_1_train, label_1_test = [label_1[i] for i in train_index], [label_1[i] for i in test_index]\n",
    "# print(len(label_1_train), sum(label_1_train))\n",
    "\n",
    "# train_test_split\n",
    "data_1 = list(zip(prompt_1, txt_1))\n",
    "data_1_train, data_1_test, label_1_train, label_1_test = train_test_split(\n",
    "    data_1, label_1, test_size=TEST_FRA, stratify=label_1, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "\n",
    "# Resampling / sub-sampling\n",
    "h_1 = []\n",
    "m_1 = []\n",
    "for i in range(len(data_1_train)):\n",
    "    if label_1_train[i] == MACHINE_IND:\n",
    "        m_1.append(data_1_train[i])\n",
    "    else:\n",
    "        h_1.append(data_1_train[i])\n",
    "n_samples = min(len(h_1), len(m_1))\n",
    "\n",
    "h_1 = resample(h_1, n_samples = n_samples, random_state=0)\n",
    "m_1 = resample(m_1, n_samples = n_samples, random_state=0)\n",
    "data_1_train = h_1+m_1\n",
    "label_1_train = [HUMAND_IND for i in range(n_samples)]+[MACHINE_IND for i in range(n_samples)]\n",
    "print(len(data_1_train))\n",
    "print(sum(label_1_train))\n",
    "data_1_train, d_t_1, label_1_train, l_t_1 = train_test_split(\n",
    "    data_1_train, label_1_train, test_size=TEST_FRA, random_state=RANDOM_SEED\n",
    ")\n",
    "data_1_train += d_t_1\n",
    "label_1_train += l_t_1\n",
    "\n",
    "\n",
    "# unzip\n",
    "prompt_1_train, txt_1_train = zip(*data_1_train)\n",
    "prompt_1_train, txt_1_train = list(prompt_1_train), list(txt_1_train)\n",
    "\n",
    "prompt_1_test, txt_1_test = zip(*data_1_test)\n",
    "prompt_1_test, txt_1_test = list(prompt_1_test), list(txt_1_test)\n",
    "\n",
    "# vectorize\n",
    "p_dv = DictVectorizer()\n",
    "t_dv = DictVectorizer()\n",
    "prompt_1_train = sparse_transf(prompt_1_train, p_dv)\n",
    "prompt_1_test = sparse_transf(prompt_1_test, p_dv, False)\n",
    "txt_1_train = sparse_transf(txt_1_train, t_dv)\n",
    "txt_1_test = sparse_transf(txt_1_test, t_dv, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes' on prompt:\n",
      "4900\n",
      "2450\n",
      "Accuracy Score:  0.7542857142857143\n",
      "Marco F1 score:  0.7539754836210791\n",
      "Precision:  0.7555748076428479\n",
      "Recall:  0.7542857142857142\n",
      "Confusion matrix: \n",
      "[[1761  689]\n",
      " [ 515 1935]]\n",
      "\n",
      "Logistic Regression on prompt: \n",
      "4900\n",
      "2450\n",
      "Accuracy Score:  0.8293877551020408\n",
      "Marco F1 score:  0.828288803444083\n",
      "Precision:  0.8380416205891223\n",
      "Recall:  0.8293877551020408\n",
      "Confusion matrix: \n",
      "[[1836  614]\n",
      " [ 222 2228]]\n",
      "\n",
      "---------------------\n",
      "Naive Bayes' on text:\n",
      "4900\n",
      "2450\n",
      "Accuracy Score:  0.7687755102040816\n",
      "Marco F1 score:  0.7664495284335089\n",
      "Precision:  0.7799269317530566\n",
      "Recall:  0.7687755102040816\n",
      "Confusion matrix: \n",
      "[[2128  322]\n",
      " [ 811 1639]]\n",
      "\n",
      "Logistic Regression on text: \n",
      "4900\n",
      "2450\n",
      "Accuracy Score:  0.9169387755102041\n",
      "Marco F1 score:  0.9168811669929167\n",
      "Precision:  0.9180978870367644\n",
      "Recall:  0.9169387755102041\n",
      "Confusion matrix: \n",
      "[[2182  268]\n",
      " [ 139 2311]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "CV_NUM = 5\n",
    "\n",
    "def vali_model(clf, d_train, label_train, cv_num = CV_NUM):\n",
    "    print(len(label_train))\n",
    "    print(sum(label_train))\n",
    "    label_pred = cross_val_predict(clf, d_train, label_train, cv=cv_num)\n",
    "    acc_score = accuracy_score(label_train, label_pred)\n",
    "    print(\"Accuracy Score: \", acc_score)\n",
    "    print(\"Marco F1 score: \", f1_score(label_train, label_pred, average=\"macro\"))\n",
    "    print(\"Precision: \", precision_score(label_train, label_pred, average=\"macro\"))\n",
    "    print(\"Recall: \", recall_score(label_train, label_pred, average=\"macro\"))\n",
    "    print(\"Confusion matrix: \")\n",
    "    print(confusion_matrix(label_train, label_pred))\n",
    "    print()\n",
    "\n",
    "\n",
    "# Naive bayes on promp\n",
    "print(\"Naive Bayes' on prompt:\")\n",
    "p_nb_clf = MultinomialNB()\n",
    "vali_model(p_nb_clf, prompt_1_train, label_1_train)\n",
    "\n",
    "print(\"Logistic Regression on prompt: \")\n",
    "p_lr_clf = LogisticRegression(max_iter=1000)\n",
    "vali_model(p_lr_clf, prompt_1_train, label_1_train)\n",
    "\n",
    "\n",
    "# Naive bayes on txt\n",
    "print(\"---------------------\")\n",
    "print(\"Naive Bayes' on text:\")\n",
    "t_nb_clf = MultinomialNB()\n",
    "vali_model(t_nb_clf, txt_1_train, label_1_train)\n",
    "\n",
    "print(\"Logistic Regression on text: \")\n",
    "t_lr_clf = LogisticRegression(max_iter=1000)\n",
    "vali_model(t_lr_clf, txt_1_train, label_1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
