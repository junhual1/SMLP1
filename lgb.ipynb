{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain1 - human:machine = 122584:3500\n",
      "Domain2 - human:machine = 100:400\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMClassifier\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load dataset\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Convert integer list to string\n",
    "def int_list_to_str(int_list):\n",
    "    return ' '.join(map(str, int_list))\n",
    "\n",
    "\n",
    "# Load data\n",
    "set1_human = load_dataset(\"./data/set1_human.json\")\n",
    "set1_machine = load_dataset(\"./data/set1_machine.json\")\n",
    "set2_human = load_dataset(\"./data/set2_human.json\")\n",
    "set2_machine = load_dataset(\"./data/set2_machine.json\")\n",
    "\n",
    "# print the ratio of human and machine\n",
    "print(\"Domain1 - human:machine = {}:{}\".format(len(set1_human), len(set1_machine)))\n",
    "print(\"Domain2 - human:machine = {}:{}\".format(len(set2_human), len(set2_machine)))\n",
    "# Label data and combine\n",
    "set1_human[\"label\"] = 1\n",
    "set1_machine[\"label\"] = 0\n",
    "set2_human[\"label\"] = 1\n",
    "set2_machine[\"label\"] = 0\n",
    "\n",
    "dataset1 = pd.concat([set1_human, set1_machine], ignore_index=True)\n",
    "dataset2 = pd.concat([set2_human, set2_machine], ignore_index=True)\n",
    "\n",
    "# Convert integer lists to strings\n",
    "dataset1['txt'] = dataset1['txt'].apply(int_list_to_str)\n",
    "dataset2['txt'] = dataset2['txt'].apply(int_list_to_str)\n",
    "dataset1['prompt'] = dataset1['prompt'].apply(int_list_to_str)\n",
    "dataset2['prompt'] = dataset2['prompt'].apply(int_list_to_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_dataset = pd.concat([dataset1, dataset2], ignore_index=True)\n",
    "whole_dataset['combined'] = whole_dataset['txt'] + ' ' + whole_dataset['prompt']\n",
    "# # Word2Vec\n",
    "# all_text = [text.split() for text in whole_dataset['combined'].values]\n",
    "# word2vec_model = Word2Vec(all_text, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# # Save model\n",
    "# word2vec_model.save(\"word2vec.model\")\n",
    "\n",
    "# Load model\n",
    "word2vec_model = Word2Vec.load(\"word2vec.model\")\n",
    "\n",
    "def vectorize(dataset, word2vec_model, has_label=True):\n",
    "    X_prompt = dataset['prompt'].values\n",
    "    X_txt = dataset['txt'].values\n",
    "    X_prompt = [text.split() for text in X_prompt]\n",
    "    X_txt = [text.split() for text in X_txt]\n",
    "    # prompt may be empty, use zero vector to represent\n",
    "    X_prompt = np.array([np.mean([word2vec_model.wv[token] for token in tokens if token in word2vec_model.wv],\n",
    "                                 axis=0) if tokens else np.zeros(100) for tokens in X_prompt])\n",
    "    X_txt = np.array([np.mean([word2vec_model.wv[token] \n",
    "                               for token in tokens if token in word2vec_model.wv], axis=0) for tokens in X_txt])\n",
    "    promt_len = np.expand_dims(np.array([len(text.split()) for text in dataset['prompt'].values]), axis=1)\n",
    "    txt_len = np.expand_dims(np.array([len(text.split()) for text in dataset['txt'].values]), axis=1)\n",
    "    X = np.concatenate((X_prompt, X_txt, promt_len, txt_len), axis=1)\n",
    "    if has_label:\n",
    "        y = dataset['label'].values\n",
    "        return X, y\n",
    "    else:\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done vectorize!\n"
     ]
    }
   ],
   "source": [
    "# Split dataset 1 and dataset 2\n",
    "dataset1_train, dataset1_test = train_test_split(\n",
    "    dataset1, test_size=0.2, random_state=42)\n",
    "\n",
    "dataset2_train, dataset2_test = train_test_split(\n",
    "    dataset2, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess dataset 1 and dataset 2\n",
    "X_train1, y_train1 = vectorize(dataset1_train, word2vec_model)\n",
    "X_test1, y_test1 = vectorize(dataset1_test, word2vec_model)\n",
    "X_train2, y_train2 = vectorize(dataset2_train, word2vec_model)\n",
    "X_test2, y_test2 = vectorize(dataset2_test, word2vec_model)\n",
    "X_train1_whole, y_train1_whole = vectorize(dataset1, word2vec_model)\n",
    "X_train2_whole, y_train2_whole = vectorize(dataset2, word2vec_model)\n",
    "print(\"done vectorize!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary on dataset 1 test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.84      0.87       691\n",
      "           1       1.00      1.00      1.00     24526\n",
      "\n",
      "    accuracy                           0.99     25217\n",
      "   macro avg       0.94      0.92      0.93     25217\n",
      "weighted avg       0.99      0.99      0.99     25217\n",
      "\n",
      "[[  583   108]\n",
      " [   72 24454]]\n",
      "F1 score:  0.9963331160365059\n"
     ]
    }
   ],
   "source": [
    "lgbm1 = LGBMClassifier(objective='binary', n_estimators=500,\n",
    "                       boosting_type='gbdt', class_weight='balanced',\n",
    "                       reg_alpha=0.5, reg_lambda=0)\n",
    "lgbm1.fit(X_train1, y_train1)\n",
    "# Predict and evaluate on dataset 1 test set\n",
    "print(\"*** Summary on dataset 1 test set:\")\n",
    "y_pred1 = lgbm1.predict(X_test1)\n",
    "print(classification_report(y_test1, y_pred1))\n",
    "print(confusion_matrix(y_test1, y_pred1))\n",
    "# f1 score\n",
    "print(\"F1 score: \", f1_score(y_test1, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Summary on dataset 2 test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.90      0.87        72\n",
      "           1       0.68      0.54      0.60        28\n",
      "\n",
      "    accuracy                           0.80       100\n",
      "   macro avg       0.76      0.72      0.73       100\n",
      "weighted avg       0.79      0.80      0.79       100\n",
      "\n",
      "[[65  7]\n",
      " [13 15]]\n",
      "F1 score:  0.6\n"
     ]
    }
   ],
   "source": [
    "# # WAY2: SMOTE for oversampling\n",
    "smote = SMOTE(random_state=90051)\n",
    "X_train2_resampled, y_train2_resampled = smote.fit_resample(X_train2, y_train2)\n",
    "\n",
    "lgbm2 = LGBMClassifier(objective='binary', n_estimators=500,\n",
    "                           boosting_type='gbdt', class_weight='balanced',\n",
    "                           reg_alpha=0.1, reg_lambda=1, learning_rate=0.01)\n",
    "lgbm2.fit(X_train2_resampled, y_train2_resampled, init_model=lgbm1)\n",
    "\n",
    "# Predict and evaluate on dataset 2 test set\n",
    "print(\"\\n*** Summary on dataset 2 test set:\")\n",
    "y_pred2 = lgbm2.predict(X_test2)\n",
    "print(classification_report(y_test2, y_pred2))\n",
    "print(confusion_matrix(y_test2, y_pred2))\n",
    "# f1 score\n",
    "print(\"F1 score: \", f1_score(y_test2, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to file: LGB_domain_adaption_l1l2_all_data.csv\n"
     ]
    }
   ],
   "source": [
    "# OUTPUT to CSV\n",
    "# use the whole dataset2 to train model2\n",
    "lgbm1 = LGBMClassifier(objective='binary', n_estimators=500,\n",
    "                       boosting_type='gbdt', class_weight='balanced',\n",
    "                       reg_alpha=0.5, reg_lambda=0)\n",
    "lgbm1.fit(X_train1_whole, y_train1_whole)\n",
    "\n",
    "smote = SMOTE(random_state=90051)\n",
    "X_train2_resampled, y_train2_resampled = smote.fit_resample(\n",
    "    X_train2_whole, y_train2_whole)\n",
    "\n",
    "lgbm2 = LGBMClassifier(objective='binary', n_estimators=500, \n",
    "                       boosting_type='gbdt', class_weight='balanced',\n",
    "                       reg_alpha=0.1, reg_lambda=1, learning_rate=0.01)\n",
    "lgbm2.fit(X_train2_resampled, y_train2_resampled, init_model=lgbm1)\n",
    "\n",
    "# Load test set\n",
    "test_data = load_dataset(\"./data/test.json\")\n",
    "test_data['txt'] = test_data['txt'].apply(int_list_to_str)\n",
    "test_data['prompt'] = test_data['prompt'].apply(int_list_to_str)\n",
    "test_data1 = test_data.iloc[:600]\n",
    "test_data2 = test_data.iloc[600:]\n",
    "\n",
    "# Preprocess test set\n",
    "X_test1 = vectorize(test_data1, word2vec_model, has_label=False)\n",
    "X_test2 = vectorize(test_data2, word2vec_model, has_label=False)\n",
    "\n",
    "y_pred1 = lgbm1.predict(X_test1)\n",
    "y_pred2 = lgbm2.predict(X_test2)\n",
    "\n",
    "# Combine predictions for both domains\n",
    "y_pred_test = np.concatenate([y_pred1, y_pred2])\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "output_df = pd.DataFrame(\n",
    "    {\"Id\": range(len(y_pred_test)), \"Predicted\": y_pred_test})\n",
    "filename = \"LGB_domain_adaption_l1l2_all_data.csv\"\n",
    "output_df.to_csv(filename, index=False)\n",
    "print(\"Predictions saved to file:\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Best parameters found:  {'boosting_type': 'gbdt', 'class_weight': 'balanced', 'objective': 'binary', 'reg_alpha': 0.5, 'reg_lambda': 0}\n",
      "Best score found:  0.9959149972679213\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning via grid search CV\n",
    "param_grid = {\n",
    "    'objective': ['binary'],\n",
    "    'boosting_type': ['gbdt'],\n",
    "    'class_weight': ['balanced'],\n",
    "    'reg_alpha': [0, 0.1, 0.3, 0.5, 0.7, 1.0],  # L1 regularization\n",
    "    'reg_lambda': [0, 0.1, 0.3, 0.5, 0.7, 1.0]  # L2 regularization\n",
    "}\n",
    "\n",
    "lgbm = LGBMClassifier(n_estimators=500)\n",
    "grid_search = GridSearchCV(estimator=lgbm, param_grid=param_grid,\n",
    "                           cv=5, scoring='f1', n_jobs=-1)\n",
    "grid_search.fit(X_train1_whole, y_train1_whole)\n",
    "# Print the best parameters and corresponding score\n",
    "print(\"Best parameters found: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary on dataset 1 test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.84      0.87       691\n",
      "           1       1.00      1.00      1.00     24526\n",
      "\n",
      "    accuracy                           0.99     25217\n",
      "   macro avg       0.94      0.92      0.93     25217\n",
      "weighted avg       0.99      0.99      0.99     25217\n",
      "\n",
      "[[  583   108]\n",
      " [   72 24454]]\n"
     ]
    }
   ],
   "source": [
    "# Train model 1 with the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "lgbm1 = LGBMClassifier(**best_params, n_estimators=500)\n",
    "lgbm1.fit(X_train1, y_train1)\n",
    "# Evaluate on dataset 1 test set\n",
    "print(\"*** Summary on dataset 1 test set:\")\n",
    "y_pred1 = lgbm1.predict(X_test1)\n",
    "print(classification_report(y_test1, y_pred1))\n",
    "print(confusion_matrix(y_test1, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'boosting_type': 'gbdt', 'class_weight': 'balanced', 'learning_rate': 0.01, 'n_estimators': 500, 'objective': 'binary', 'reg_alpha': 0.1, 'reg_lambda': 1}\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune lgbm1 on dataset 2 with a smaller learning rate\n",
    "# grid search for best alpha\n",
    "# Set up the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [500],\n",
    "    'learning_rate': [0.01],\n",
    "    'objective': ['binary'],\n",
    "    'boosting_type': ['gbdt'],\n",
    "    'class_weight': ['balanced'],\n",
    "    'reg_alpha': [0, 0.1, 0.5, 0.7, 1],\n",
    "    'reg_lambda': [0, 0.1, 0.5, 0.7, 1]\n",
    "}\n",
    "\n",
    "# Create a LightGBM model for fine-tuning\n",
    "lgbm_finetuned = LGBMClassifier()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(lgbm_finetuned, param_grid, scoring='f1', cv=5)\n",
    "\n",
    "X_train2_resampled, y_train2_resampled = smote.fit_resample(\n",
    "    X_train2_whole, y_train2_whole)\n",
    "\n",
    "grid_search.fit(X_train2_resampled, y_train2_resampled, init_model=lgbm1)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters found:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary on resampled dataset 2 test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.94      0.90        72\n",
      "           1       0.81      0.61      0.69        28\n",
      "\n",
      "    accuracy                           0.85       100\n",
      "   macro avg       0.84      0.78      0.80       100\n",
      "weighted avg       0.85      0.85      0.84       100\n",
      "\n",
      "[[68  4]\n",
      " [11 17]]\n",
      "F1 score: 0.6938775510204083\n"
     ]
    }
   ],
   "source": [
    "smote = SMOTE(random_state=90051)\n",
    "X_train2_resampled, y_train2_resampled = smote.fit_resample(X_train2, y_train2)\n",
    "\n",
    "lgbm_best = LGBMClassifier(**best_params)\n",
    "lgbm_best.fit(X_train2_resampled, y_train2_resampled)\n",
    "\n",
    "# Evaluate on resampled dataset 2 test set\n",
    "print(\"*** Summary on resampled dataset 2 test set:\")\n",
    "y_pred2 = lgbm_best.predict(X_test2)\n",
    "print(classification_report(y_test2, y_pred2))\n",
    "print(confusion_matrix(y_test2, y_pred2))\n",
    "print(\"F1 score:\", f1_score(y_test2, y_pred2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
