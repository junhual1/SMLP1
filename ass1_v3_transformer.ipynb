{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-22 01:09:47.473139: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-22 01:09:47.575045: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-22 01:09:48.660689: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Using GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-22 01:09:50.568737: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-22 01:09:50.700331: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-22 01:09:50.701287: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import backend\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "from keras import layers\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "NUM_TOKEN = 5000\n",
    "MAX_PRO_LEN = 64\n",
    "MAX_TXT_LEN = 256\n",
    "NO_EPO = 60\n",
    "NO_BAT = 128\n",
    "\n",
    "MACHINE_1_P = \"./data/set1_machine.json\"\n",
    "HUMAN_1_P = \"./data/set1_human.json\"\n",
    "MACHINE_2_P = \"./data/set2_machine.json\"\n",
    "HUMAN_2_P = \"./data/set2_human.json\"\n",
    "TEST_P = \"./data/test.json\"\n",
    "RANDOM_SEED = 42\n",
    "MACHINE_IND = 0\n",
    "HUMAN_IND = 1\n",
    "TEST_FRA = 0.01\n",
    "\n",
    "class DomainData:\n",
    "    \"\"\"\n",
    "    train_test_split, pad_sequence, PCA, class_weight\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def t_t_spli(self, test_size, random_state):\n",
    "        self.random_state = random_state\n",
    "        self.train_x, self.test_x, self.train_y, self.test_y = train_test_split(self.x, self.y, test_size=test_size, stratify = self.y, random_state = random_state)\n",
    "        self.train_x = self.train_x.reset_index(drop=True)\n",
    "        self.train_y = self.train_y.reset_index(drop=True)\n",
    "        self.test_x = self.test_x.reset_index(drop=True)\n",
    "        self.test_y = self.test_y.reset_index(drop=True)\n",
    "        \n",
    "\n",
    "    def add_padding(self, padding, prompt_len, txt_len):\n",
    "        self.train_prompt = self.train_x[\"prompt\"]\n",
    "        self.train_txt = self.train_x[\"txt\"]\n",
    "        self.train_label = self.train_y.to_numpy()\n",
    "        self.test_prompt = self.test_x[\"prompt\"]\n",
    "        self.test_txt = self.test_x[\"txt\"]\n",
    "        self.test_label = self.test_y.to_numpy()\n",
    "        unique_classes = np.unique(self.train_label)\n",
    "        class_weights = class_weight.compute_class_weight(\"balanced\", classes=unique_classes, y=self.train_y)\n",
    "        self.class_weights = dict(zip(unique_classes, class_weights))\n",
    "        \n",
    "        self.prompt_len = prompt_len\n",
    "        self.txt_len = txt_len\n",
    "        \n",
    "        self.train_prompt = pad_sequences(self.train_prompt, padding=padding, maxlen=prompt_len)\n",
    "        self.train_txt = pad_sequences(self.train_txt, padding=padding, maxlen=txt_len)\n",
    "        self.test_prompt = pad_sequences(self.test_prompt, padding=padding, maxlen=prompt_len)\n",
    "        self.test_txt = pad_sequences(self.test_txt, padding=padding, maxlen=txt_len)\n",
    "        \n",
    "        \n",
    "    def down_sampling(self):\n",
    "        mac_ind = self.train_y[self.train_y == MACHINE_IND].index.to_list()\n",
    "        hum_ind = self.train_y[self.train_y == HUMAN_IND].index.to_list()\n",
    "        lower = min(len(mac_ind), len(hum_ind))\n",
    "        sel_lit = mac_ind[:lower] + hum_ind[:lower]\n",
    "        self.train_x = self.train_x.iloc[sel_lit]\n",
    "        self.train_y = self.train_y.iloc[sel_lit]\n",
    "        random.shuffle(sel_lit)\n",
    "\n",
    "    def over_sampling(self, upper_fra):\n",
    "        mac_ind = self.train_y[self.train_y == MACHINE_IND].index.to_list()\n",
    "        hum_ind = self.train_y[self.train_y == HUMAN_IND].index.to_list()\n",
    "        lower = min(len(mac_ind), len(hum_ind))\n",
    "        if lower == len(mac_ind):\n",
    "            upper = int(lower*upper_fra) if lower*upper_fra < len(hum_ind) else len(hum_ind)\n",
    "            major = hum_ind[:upper]\n",
    "            minor = mac_ind[:lower]\n",
    "    \n",
    "        else:\n",
    "            upper = int(lower*upper_fra) if lower*upper_fra < len(mac_ind) else len(mac_ind)\n",
    "            major = mac_ind[:upper]\n",
    "            minor = hum_ind[:lower]\n",
    "        \n",
    "        add_n = upper - lower\n",
    "        oversampled = []\n",
    "        while(len(oversampled) < add_n):\n",
    "            oversampled.append(random.choice(mac_ind))\n",
    "        sel_lit = major + minor + oversampled\n",
    "        random.shuffle(sel_lit)\n",
    "        \n",
    "        self.train_x = self.train_x.iloc[sel_lit]\n",
    "        self.train_y = self.train_y.iloc[sel_lit]\n",
    "    \n",
    "    def test_down(self, frac = 1):\n",
    "        mac_ind = self.test_y[self.test_y == MACHINE_IND].index.to_list()\n",
    "        hum_ind = self.test_y[self.test_y == HUMAN_IND].index.to_list()\n",
    "        lower = min(len(mac_ind), len(hum_ind))\n",
    "        if frac > 1:\n",
    "            sel_lit = mac_ind[:lower] + hum_ind[:int(lower/frac)]\n",
    "        else:\n",
    "            sel_lit = mac_ind[:int(lower*frac)] + hum_ind[:lower]\n",
    "        self.test_x = self.test_x.iloc[sel_lit]\n",
    "        self.test_y = self.test_y.iloc[sel_lit]\n",
    "        random.shuffle(sel_lit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    # Calculate precision and recall\n",
    "    tp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)))\n",
    "    fp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)))\n",
    "    fn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)))\n",
    "    precision = tp / (tp + fp + backend.epsilon())\n",
    "    recall = tp / (tp + fn + backend.epsilon())\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1_score = 2 * precision * recall / (precision + recall + backend.epsilon())\n",
    "    \n",
    "    # Return negative F1 score as the loss (to minimize it)\n",
    "    return -f1_score\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Layer\n",
    "from tensorflow.keras.layers import Embedding, Input, GlobalAveragePooling1D, Dense, Concatenate\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = Sequential(\n",
    "            [Dense(ff_dim, activation=\"relu\"), \n",
    "             Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class TokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience = 3)\n",
    "random.seed(RANDOM_SEED)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "if len(tf.config.list_physical_devices('GPU')):\n",
    "    tf.config.list_physical_devices('GPU')\n",
    "    print(\"Using GPU\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain 2 weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_fra = 1.6\n",
    "weight_fra = 300\n",
    "\n",
    "## _______________ Read data from domain 1 _______________\n",
    "man_1_df = pd.read_json(HUMAN_1_P)\n",
    "man_1_df[\"label\"] = HUMAN_IND\n",
    "mac_1_df = pd.read_json(MACHINE_1_P).drop(\"machine_id\", axis = 1)\n",
    "mac_1_df[\"label\"] = MACHINE_IND\n",
    "domain_1_df = pd.concat([man_1_df, mac_1_df])\n",
    "\n",
    "domain_1 = DomainData(domain_1_df[[\"prompt\", \"txt\"]], domain_1_df[\"label\"])\n",
    "domain_1.t_t_spli(TEST_FRA, RANDOM_SEED)\n",
    "# domain_1.down_sampling()\n",
    "domain_1.add_padding('post', MAX_PRO_LEN, MAX_TXT_LEN)\n",
    "\n",
    "## _______________ Read data from domain 2 _______________\n",
    "man_2_df = pd.read_json(HUMAN_2_P)\n",
    "man_2_df[\"label\"] = HUMAN_IND\n",
    "mac_2_df = pd.read_json(MACHINE_2_P).drop(\"machine_id\", axis = 1)\n",
    "mac_2_df[\"label\"] = MACHINE_IND\n",
    "domain_2_df = pd.concat([man_2_df, mac_2_df])\n",
    "\n",
    "domain_2 = DomainData(domain_2_df[[\"prompt\", \"txt\"]], domain_2_df[\"label\"])\n",
    "domain_2.t_t_spli(TEST_FRA, RANDOM_SEED)\n",
    "# domain_2.over_sampling(over_fra)\n",
    "domain_2.test_down()\n",
    "domain_2.add_padding('post', MAX_PRO_LEN, MAX_TXT_LEN)\n",
    "\n",
    "\n",
    "\n",
    "## _______________ weight data _______________\n",
    "sample_weight_1 = np.ones(len(domain_1.train_label))\n",
    "sample_weight_2 = np.ones(len(domain_2.train_label))\n",
    "sample_weight_2 *= weight_fra\n",
    "sample_weight = np.concatenate([sample_weight_1, sample_weight_2])\n",
    "\n",
    "train_prompt = np.concatenate([domain_1.train_prompt, domain_2.train_prompt])\n",
    "train_txt = np.concatenate([domain_1.train_txt, domain_2.train_txt])\n",
    "train_label = np.concatenate([domain_1.train_label, domain_2.train_label])\n",
    "\n",
    "data = list(zip(train_prompt, train_txt, train_label, sample_weight))\n",
    "random.shuffle(data)\n",
    "\n",
    "train_prompt, train_txt, train_label, sample_weight = zip(*data)\n",
    "train_prompt = np.array(train_prompt)\n",
    "train_txt = np.array(train_txt)\n",
    "train_label = np.array(train_label)\n",
    "sample_weight = np.array(sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-22 00:27:59.624895: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-22 00:27:59.625503: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-22 00:27:59.626109: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-22 00:28:00.579130: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-22 00:28:00.579818: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-22 00:28:00.579836: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-04-22 00:28:00.580417: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-22 00:28:00.580463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3383 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-22 00:28:19.638271: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-04-22 00:28:20.002139: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x4e707280 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-04-22 00:28:20.002172: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2023-04-22 00:28:20.006800: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-04-22 00:28:20.217638: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-04-22 00:28:20.333364: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784/784 [==============================] - ETA: 0s - loss: 0.9038 - accuracy: 0.9446 - f1_loss: -0.9690WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 132s 160ms/step - loss: 0.9038 - accuracy: 0.9446 - f1_loss: -0.9690 - val_loss: 0.9351 - val_accuracy: 0.9619 - val_f1_loss: -0.9803\n",
      "Epoch 2/60\n",
      "784/784 [==============================] - ETA: 0s - loss: 0.5980 - accuracy: 0.9709 - f1_loss: -0.9851WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 90s 115ms/step - loss: 0.5980 - accuracy: 0.9709 - f1_loss: -0.9851 - val_loss: 0.4858 - val_accuracy: 0.9735 - val_f1_loss: -0.9865\n",
      "Epoch 3/60\n",
      "784/784 [==============================] - ETA: 0s - loss: 0.3508 - accuracy: 0.9693 - f1_loss: -0.9840WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 88s 112ms/step - loss: 0.3508 - accuracy: 0.9693 - f1_loss: -0.9840 - val_loss: 0.2625 - val_accuracy: 0.9815 - val_f1_loss: -0.9905\n",
      "Epoch 4/60\n",
      "784/784 [==============================] - ETA: 0s - loss: 0.1025 - accuracy: 0.9790 - f1_loss: -0.9892WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 89s 113ms/step - loss: 0.1025 - accuracy: 0.9790 - f1_loss: -0.9892 - val_loss: 0.2376 - val_accuracy: 0.9818 - val_f1_loss: -0.9907\n",
      "Epoch 5/60\n",
      "784/784 [==============================] - ETA: 0s - loss: 0.0568 - accuracy: 0.9834 - f1_loss: -0.9914WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 86s 110ms/step - loss: 0.0568 - accuracy: 0.9834 - f1_loss: -0.9914 - val_loss: 0.2098 - val_accuracy: 0.9833 - val_f1_loss: -0.9914\n",
      "Epoch 6/60\n",
      "784/784 [==============================] - ETA: 0s - loss: 0.0464 - accuracy: 0.9858 - f1_loss: -0.9927WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 87s 110ms/step - loss: 0.0464 - accuracy: 0.9858 - f1_loss: -0.9927 - val_loss: 0.1865 - val_accuracy: 0.9845 - val_f1_loss: -0.9920\n",
      "Epoch 7/60\n",
      "784/784 [==============================] - ETA: 0s - loss: 0.1583 - accuracy: 0.9789 - f1_loss: -0.9890WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 87s 110ms/step - loss: 0.1583 - accuracy: 0.9789 - f1_loss: -0.9890 - val_loss: 0.2024 - val_accuracy: 0.9819 - val_f1_loss: -0.9907\n",
      "Epoch 8/60\n",
      "784/784 [==============================] - ETA: 0s - loss: 0.0837 - accuracy: 0.9828 - f1_loss: -0.9911WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 86s 110ms/step - loss: 0.0837 - accuracy: 0.9828 - f1_loss: -0.9911 - val_loss: 0.2197 - val_accuracy: 0.9789 - val_f1_loss: -0.9892\n",
      "Epoch 9/60\n",
      "784/784 [==============================] - ETA: 0s - loss: 0.0419 - accuracy: 0.9868 - f1_loss: -0.9932WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 87s 110ms/step - loss: 0.0419 - accuracy: 0.9868 - f1_loss: -0.9932 - val_loss: 0.2246 - val_accuracy: 0.9844 - val_f1_loss: -0.9919\n",
      "Epoch 10/60\n",
      "784/784 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.9892 - f1_loss: -0.9944WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 87s 111ms/step - loss: 0.0335 - accuracy: 0.9892 - f1_loss: -0.9944 - val_loss: 0.2426 - val_accuracy: 0.9854 - val_f1_loss: -0.9924\n",
      "Epoch 11/60\n",
      "784/784 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9913 - f1_loss: -0.9955WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 87s 111ms/step - loss: 0.0261 - accuracy: 0.9913 - f1_loss: -0.9955 - val_loss: 0.1382 - val_accuracy: 0.9860 - val_f1_loss: -0.9928\n",
      "Epoch 12/60\n",
      "784/784 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9924 - f1_loss: -0.9961WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 87s 111ms/step - loss: 0.0223 - accuracy: 0.9924 - f1_loss: -0.9961 - val_loss: 0.1393 - val_accuracy: 0.9872 - val_f1_loss: -0.9934\n",
      "Epoch 13/60\n",
      "784/784 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9933 - f1_loss: -0.9966WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 87s 110ms/step - loss: 0.0189 - accuracy: 0.9933 - f1_loss: -0.9966 - val_loss: 0.1742 - val_accuracy: 0.9877 - val_f1_loss: -0.9936\n",
      "Epoch 14/60\n",
      "784/784 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9942 - f1_loss: -0.9970WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 86s 110ms/step - loss: 0.0165 - accuracy: 0.9942 - f1_loss: -0.9970 - val_loss: 0.2071 - val_accuracy: 0.9877 - val_f1_loss: -0.9937\n",
      "Epoch 15/60\n",
      "784/784 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9950 - f1_loss: -0.9974WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 86s 110ms/step - loss: 0.0141 - accuracy: 0.9950 - f1_loss: -0.9974 - val_loss: 0.2029 - val_accuracy: 0.9874 - val_f1_loss: -0.9935\n",
      "Epoch 16/60\n",
      "784/784 [==============================] - ETA: 0s - loss: 0.0931 - accuracy: 0.9872 - f1_loss: -0.9932WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 87s 111ms/step - loss: 0.0931 - accuracy: 0.9872 - f1_loss: -0.9932 - val_loss: 0.4514 - val_accuracy: 0.9834 - val_f1_loss: -0.9914\n",
      "Epoch 17/60\n",
      "784/784 [==============================] - ETA: 0s - loss: 0.1337 - accuracy: 0.9862 - f1_loss: -0.9929WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 86s 110ms/step - loss: 0.1337 - accuracy: 0.9862 - f1_loss: -0.9929 - val_loss: 0.1911 - val_accuracy: 0.9862 - val_f1_loss: -0.9929\n",
      "Epoch 18/60\n",
      "784/784 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9928 - f1_loss: -0.9963WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 87s 110ms/step - loss: 0.0205 - accuracy: 0.9928 - f1_loss: -0.9963 - val_loss: 0.2079 - val_accuracy: 0.9877 - val_f1_loss: -0.9936\n",
      "Model Saved: trans_model_weighted.h5\n",
      "Model Loaded: trans_model_weighted.h5\n",
      "loss:  0.0002441598626319319\n",
      "accuracy 1.0\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[[1 0]\n",
      " [0 1]]\n",
      "f1-score:  1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Layer\n",
    "from tensorflow.keras.layers import Embedding, Input, GlobalAveragePooling1D, Dense, Concatenate\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
    "\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = Sequential(\n",
    "            [Dense(ff_dim, activation=\"relu\"), \n",
    "             Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class TokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "vocab_size = 5000  # Only consider the top 20k words\n",
    "embed_dim = 128  # Embedding size for each token\n",
    "num_heads = 8  # Number of attention heads\n",
    "ff_dim = 20  # Hidden layer size in feed forward network inside transformer\n",
    "epo_size = NO_EPO\n",
    "batch_size = 128\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "\n",
    "# define model\n",
    "inputs_p = Input(shape=(MAX_PRO_LEN,))\n",
    "embedding_layer = TokenAndPositionEmbedding(MAX_PRO_LEN, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs_p)\n",
    "x = transformer_block(x)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(20, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "\n",
    "# define model\n",
    "inputs_t = Input(shape=(MAX_TXT_LEN,))\n",
    "embedding_layer = TokenAndPositionEmbedding(MAX_TXT_LEN, vocab_size, embed_dim)\n",
    "y = embedding_layer(inputs_t)\n",
    "y = transformer_block(y)\n",
    "y = GlobalAveragePooling1D()(y)\n",
    "y = Dropout(0.1)(y)\n",
    "y = Dense(20, activation=\"relu\")(y)\n",
    "y = Dropout(0.1)(y)\n",
    "\n",
    "\n",
    "# Concatenate outputs from prompt and text models\n",
    "merged = Concatenate()([x, y])\n",
    "merged = Dense(units=64, activation='relu')(merged)\n",
    "merged = Dense(20, activation=\"relu\")(merged)\n",
    "outputs = Dense(units=1, activation='sigmoid')(merged)\n",
    "trans_model_2 = Model(inputs=[inputs_p, inputs_t], outputs=outputs)\n",
    "\n",
    "# Compile and train\n",
    "trans_model_2.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\", f1_loss])\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('trans_model_weighted.h5', monitor='val_loss', save_best_only=True)\n",
    "trans_model_2.fit([train_prompt, train_txt], train_label, epochs=epo_size, batch_size=batch_size, sample_weight = sample_weight, validation_split=0.2, callbacks = [callback, model_checkpoint])\n",
    "print(\"Model Saved: trans_model_weighted.h5\")\n",
    "\n",
    "trans_model_2 = tf.keras.models.load_model(\"trans_model_weighted.h5\", custom_objects={ 'f1_loss': f1_loss, 'TransformerBlock': TransformerBlock, 'TokenAndPositionEmbedding':TokenAndPositionEmbedding })\n",
    "print(\"Model Loaded: trans_model_weighted.h5\")\n",
    "loss, accuracy, f1 = trans_model_2.evaluate([domain_2.test_prompt, domain_2.test_txt], domain_2.test_label, verbose=False)\n",
    "print(\"loss: \", loss)\n",
    "print(\"accuracy\", accuracy)\n",
    "trans_2_pre_rnn = trans_model_2.predict([domain_2.test_prompt, domain_2.test_txt])\n",
    "trans_2_pre_rnn = np.round(trans_2_pre_rnn).flatten()\n",
    "confusion = confusion_matrix(domain_2.test_label, trans_2_pre_rnn)\n",
    "# trans_2_pre_rnn = [0 if i.flatten()[0] > i.flatten()[1] else 1 for i in trans_2_pre_rnn]\n",
    "# confusion = confusion_matrix(domain_2.test_label, trans_2_pre_rnn)\n",
    "print(confusion)\n",
    "f1 = f1_score(domain_2.test_label, trans_2_pre_rnn)\n",
    "print(\"f1-score: \", f1)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## _______________ Read data from domain 1 _______________\n",
    "man_1_df = pd.read_json(HUMAN_1_P)\n",
    "man_1_df[\"label\"] = HUMAN_IND\n",
    "mac_1_df = pd.read_json(MACHINE_1_P).drop(\"machine_id\", axis = 1)\n",
    "mac_1_df[\"label\"] = MACHINE_IND\n",
    "domain_1_df = pd.concat([man_1_df, mac_1_df])\n",
    "\n",
    "domain_1 = DomainData(domain_1_df[[\"prompt\", \"txt\"]], domain_1_df[\"label\"])\n",
    "domain_1.t_t_spli(TEST_FRA, RANDOM_SEED)\n",
    "domain_1.over_sampling(1.6)\n",
    "domain_1.test_down()\n",
    "domain_1.add_padding('post', MAX_PRO_LEN, MAX_TXT_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-22 01:09:54.344770: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-22 01:09:54.345259: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-22 01:09:54.345650: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-22 01:09:55.522950: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-22 01:09:55.523678: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-22 01:09:55.523706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-04-22 01:09:55.524080: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-22 01:09:55.524654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3383 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-22 01:10:00.443255: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-04-22 01:10:00.520292: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x4dd54eb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-04-22 01:10:00.520331: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2023-04-22 01:10:00.525595: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-04-22 01:10:00.766116: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-04-22 01:10:00.875155: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 26s 277ms/step - loss: 0.3453 - accuracy: 0.8589 - f1_loss: -0.8519 - val_loss: 0.2620 - val_accuracy: 0.8954 - val_f1_loss: -0.8867\n",
      "Epoch 2/60\n",
      "70/70 [==============================] - 13s 189ms/step - loss: 0.2038 - accuracy: 0.9191 - f1_loss: -0.9168 - val_loss: 0.1832 - val_accuracy: 0.9301 - val_f1_loss: -0.9294\n",
      "Epoch 3/60\n",
      "70/70 [==============================] - 11s 152ms/step - loss: 0.1063 - accuracy: 0.9604 - f1_loss: -0.9602 - val_loss: 0.2691 - val_accuracy: 0.9152 - val_f1_loss: -0.9103\n",
      "Epoch 4/60\n",
      "70/70 [==============================] - 8s 113ms/step - loss: 0.0536 - accuracy: 0.9814 - f1_loss: -0.9817 - val_loss: 0.1577 - val_accuracy: 0.9500 - val_f1_loss: -0.9484\n",
      "Epoch 5/60\n",
      "70/70 [==============================] - 9s 128ms/step - loss: 0.0297 - accuracy: 0.9910 - f1_loss: -0.9910 - val_loss: 0.1786 - val_accuracy: 0.9504 - val_f1_loss: -0.9484\n",
      "Epoch 6/60\n",
      "70/70 [==============================] - 6s 80ms/step - loss: 0.0425 - accuracy: 0.9850 - f1_loss: -0.9852 - val_loss: 0.1887 - val_accuracy: 0.9504 - val_f1_loss: -0.9478\n",
      "Epoch 7/60\n",
      "70/70 [==============================] - 6s 90ms/step - loss: 0.0171 - accuracy: 0.9943 - f1_loss: -0.9943 - val_loss: 0.1912 - val_accuracy: 0.9522 - val_f1_loss: -0.9513\n",
      "Epoch 8/60\n",
      "70/70 [==============================] - 5s 71ms/step - loss: 0.0092 - accuracy: 0.9970 - f1_loss: -0.9969 - val_loss: 0.2520 - val_accuracy: 0.9432 - val_f1_loss: -0.9400\n",
      "Epoch 9/60\n",
      "70/70 [==============================] - 5s 69ms/step - loss: 0.0050 - accuracy: 0.9984 - f1_loss: -0.9984 - val_loss: 0.2292 - val_accuracy: 0.9585 - val_f1_loss: -0.9568\n",
      "Epoch 10/60\n",
      "70/70 [==============================] - 5s 69ms/step - loss: 8.0989e-04 - accuracy: 0.9999 - f1_loss: -0.9999 - val_loss: 0.3772 - val_accuracy: 0.9382 - val_f1_loss: -0.9361\n",
      "Epoch 11/60\n",
      "70/70 [==============================] - 5s 65ms/step - loss: 0.0018 - accuracy: 0.9997 - f1_loss: -0.9997 - val_loss: 0.3181 - val_accuracy: 0.9482 - val_f1_loss: -0.9470\n",
      "Epoch 12/60\n",
      "70/70 [==============================] - 4s 60ms/step - loss: 0.0021 - accuracy: 0.9994 - f1_loss: -0.9994 - val_loss: 0.5134 - val_accuracy: 0.9270 - val_f1_loss: -0.9238\n",
      "Epoch 13/60\n",
      "70/70 [==============================] - 5s 66ms/step - loss: 0.0087 - accuracy: 0.9973 - f1_loss: -0.9971 - val_loss: 0.2622 - val_accuracy: 0.9450 - val_f1_loss: -0.9449\n",
      "Model Saved: trans_model.h5\n",
      "Model Loaded: trans_model.h5\n",
      "loss:  0.3119431436061859\n",
      "accuracy 0.9142857193946838\n",
      "3/3 [==============================] - 0s 7ms/step\n",
      "[[32  3]\n",
      " [ 3 32]]\n",
      "f1-score:  0.9142857142857143\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Layer\n",
    "from tensorflow.keras.layers import Embedding, Input, GlobalAveragePooling1D, Dense, Concatenate\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
    "\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = Sequential(\n",
    "            [Dense(ff_dim, activation=\"relu\"), \n",
    "             Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class TokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "vocab_size = 5000  # Only consider the top 20k words\n",
    "embed_dim = 128  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 64  # Hidden layer size in feed forward network inside transformer\n",
    "epo_size = NO_EPO\n",
    "batch_size = 128\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "\n",
    "# define model\n",
    "inputs_p = Input(shape=(MAX_PRO_LEN,))\n",
    "embedding_layer = TokenAndPositionEmbedding(MAX_PRO_LEN, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs_p)\n",
    "x = transformer_block(x)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(20, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "\n",
    "# define model\n",
    "inputs_t = Input(shape=(MAX_TXT_LEN,))\n",
    "embedding_layer = TokenAndPositionEmbedding(MAX_TXT_LEN, vocab_size, embed_dim)\n",
    "y = embedding_layer(inputs_t)\n",
    "y = transformer_block(y)\n",
    "y = GlobalAveragePooling1D()(y)\n",
    "y = Dropout(0.1)(y)\n",
    "y = Dense(20, activation=\"relu\")(y)\n",
    "y = Dropout(0.1)(y)\n",
    "\n",
    "\n",
    "# Concatenate outputs from prompt and text models\n",
    "merged = Concatenate()([x, y])\n",
    "merged = Dense(units=64, activation='relu')(merged)\n",
    "merged = Dense(20, activation=\"relu\")(merged)\n",
    "outputs = Dense(units=1, activation='sigmoid')(merged)\n",
    "trans_model_2 = Model(inputs=[inputs_p, inputs_t], outputs=outputs)\n",
    "\n",
    "# Compile and train\n",
    "trans_model_2.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\", f1_loss])\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('trans_model.h5', monitor='val_loss', save_best_only=True)\n",
    "trans_model_2.fit([domain_1.train_prompt, domain_1.train_txt], domain_1.train_label, epochs=epo_size, batch_size=batch_size, validation_split=0.2, callbacks = [callback, model_checkpoint])\n",
    "print(\"Model Saved: trans_model.h5\")\n",
    "\n",
    "trans_model_2 = tf.keras.models.load_model(\"trans_model.h5\", custom_objects={ 'f1_loss': f1_loss, 'TransformerBlock': TransformerBlock, 'TokenAndPositionEmbedding':TokenAndPositionEmbedding })\n",
    "print(\"Model Loaded: trans_model.h5\")\n",
    "loss, accuracy, f1 = trans_model_2.evaluate([domain_1.test_prompt, domain_1.test_txt], domain_1.test_label, verbose=False)\n",
    "print(\"loss: \", loss)\n",
    "print(\"accuracy\", accuracy)\n",
    "trans_1_pre_rnn = trans_model_2.predict([domain_1.test_prompt, domain_1.test_txt])\n",
    "trans_1_pre_rnn = np.round(trans_1_pre_rnn).flatten()\n",
    "confusion = confusion_matrix(domain_1.test_label, trans_1_pre_rnn)\n",
    "# trans_1_pre_rnn = [0 if i.flatten()[0] > i.flatten()[1] else 1 for i in trans_1_pre_rnn]\n",
    "# confusion = confusion_matrix(domain_1.test_label, trans_1_pre_rnn)\n",
    "print(confusion)\n",
    "f1 = f1_score(domain_1.test_label, trans_1_pre_rnn)\n",
    "print(\"f1-score: \", f1)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 12ms/step\n",
      "13/13 [==============================] - 0s 18ms/step\n"
     ]
    }
   ],
   "source": [
    "DOMAIN_SPL = 600\n",
    "\n",
    "test_df = pd.read_json(TEST_P)\n",
    "test_prompt = pad_sequences(test_df[\"prompt\"], padding=\"post\", maxlen=MAX_PRO_LEN)\n",
    "test_txt = pad_sequences(test_df[\"txt\"], padding=\"post\", maxlen=MAX_TXT_LEN)\n",
    "\n",
    "model_1 = tf.keras.models.load_model(\"trans_model.h5\", custom_objects={ 'f1_loss': f1_loss, 'TransformerBlock': TransformerBlock, 'TokenAndPositionEmbedding':TokenAndPositionEmbedding })\n",
    "model_2 = tf.keras.models.load_model(\"trans_model_weighted.h5\", custom_objects={ 'f1_loss': f1_loss, 'TransformerBlock': TransformerBlock, 'TokenAndPositionEmbedding':TokenAndPositionEmbedding })\n",
    "\n",
    "pred = []\n",
    "pred += model_1.predict([test_prompt[:DOMAIN_SPL], test_txt[:DOMAIN_SPL]]).tolist()\n",
    "pred += model_2.predict([test_prompt[DOMAIN_SPL:], test_txt[DOMAIN_SPL:]]).tolist()\n",
    "pred = [int(i) for i in np.round(pred).flatten()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(pred)\n",
    "pred_df.columns = [\"Predicted\"]\n",
    "pred_df.index.names = ['Id']\n",
    "\n",
    "pred_df.to_csv(\"./data/result3.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
