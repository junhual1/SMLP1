{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 13:43:46.519529: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-27 13:43:46.695769: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-27 13:43:47.392455: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Using GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 13:43:48.651450: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-27 13:43:48.758954: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-27 13:43:48.759502: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Layer, Embedding, Input, GlobalAveragePooling1D, Dense, Concatenate\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "NUM_TOKEN = 5000\n",
    "MAX_PRO_LEN = 64\n",
    "MAX_TXT_LEN = 256\n",
    "NO_EPO = 60\n",
    "NO_BAT = 128\n",
    "\n",
    "MACHINE_1_P = \"./data/set1_machine.json\"\n",
    "HUMAN_1_P = \"./data/set1_human.json\"\n",
    "MACHINE_2_P = \"./data/set2_machine.json\"\n",
    "HUMAN_2_P = \"./data/set2_human.json\"\n",
    "TEST_P = \"./data/test.json\"\n",
    "RANDOM_SEED = 42\n",
    "MACHINE_IND = 0\n",
    "HUMAN_IND = 1\n",
    "TEST_FRA = 0.2\n",
    "\n",
    "\n",
    "class DomainData:\n",
    "    \"\"\"\n",
    "    Domain dataset contains data for traininig. \n",
    "    Featured with function train test split, padding, \n",
    "    downsampling, oversampling and rebalance test class weight.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        # inupts are (pd.Dataframe, pd.Series)\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def t_t_spli(self, test_size, random_state):\n",
    "        ## train test split according to test fraction <test_size> and random state <random_state>\n",
    "        ## generated train_x / test_x are pd.Dataframe, train_y / test_y are pd.Series\n",
    "        self.random_state = random_state\n",
    "        self.train_x, self.test_x, self.train_y, self.test_y = train_test_split(\n",
    "            self.x, self.y, test_size=test_size, stratify=self.y, random_state=random_state)\n",
    "        self.train_x = self.train_x.reset_index(drop=True)\n",
    "        self.train_y = self.train_y.reset_index(drop=True)\n",
    "        self.test_x = self.test_x.reset_index(drop=True)\n",
    "        self.test_y = self.test_y.reset_index(drop=True)\n",
    "\n",
    "    def add_padding(self, padding, prompt_len, txt_len):\n",
    "        ## add padding of the given length\n",
    "        ## out put are np arraies\n",
    "        self.train_prompt = self.train_x[\"prompt\"]\n",
    "        self.train_txt = self.train_x[\"txt\"]\n",
    "        self.train_label = self.train_y.to_numpy()\n",
    "        self.test_prompt = self.test_x[\"prompt\"]\n",
    "        self.test_txt = self.test_x[\"txt\"]\n",
    "        self.test_label = self.test_y.to_numpy()\n",
    "        unique_classes = np.unique(self.train_label)\n",
    "        class_weights = class_weight.compute_class_weight(\n",
    "            \"balanced\", classes=unique_classes, y=self.train_y)\n",
    "        self.class_weights = dict(zip(unique_classes, class_weights))\n",
    "\n",
    "        self.prompt_len = prompt_len\n",
    "        self.txt_len = txt_len\n",
    "\n",
    "        self.train_prompt = pad_sequences(\n",
    "            self.train_prompt, padding=padding, maxlen=prompt_len)\n",
    "        self.train_txt = pad_sequences(\n",
    "            self.train_txt, padding=padding, maxlen=txt_len)\n",
    "        self.test_prompt = pad_sequences(\n",
    "            self.test_prompt, padding=padding, maxlen=prompt_len)\n",
    "        self.test_txt = pad_sequences(\n",
    "            self.test_txt, padding=padding, maxlen=txt_len)\n",
    "\n",
    "    def down_sampling(self):\n",
    "        ## down sample the majority calss to have same number of record compare to nimor class\n",
    "        mac_ind = self.train_y[self.train_y == MACHINE_IND].index.to_list()\n",
    "        hum_ind = self.train_y[self.train_y == HUMAN_IND].index.to_list()\n",
    "        lower = min(len(mac_ind), len(hum_ind))\n",
    "        sel_lit = mac_ind[:lower] + hum_ind[:lower]\n",
    "        self.train_x = self.train_x.iloc[sel_lit]\n",
    "        self.train_y = self.train_y.iloc[sel_lit]\n",
    "        random.shuffle(sel_lit)\n",
    "\n",
    "    def over_sampling(self, upper_fra):\n",
    "        ## over sampling the minority class with a fraction then \n",
    "        ### down sample the majority to have the same number of records\n",
    "        \n",
    "        # find index\n",
    "        mac_ind = self.train_y[self.train_y == MACHINE_IND].index.to_list()\n",
    "        hum_ind = self.train_y[self.train_y == HUMAN_IND].index.to_list()\n",
    "        lower = min(len(mac_ind), len(hum_ind))\n",
    "        if lower == len(mac_ind):\n",
    "            upper = int(lower*upper_fra) if lower * \\\n",
    "                upper_fra < len(hum_ind) else len(hum_ind)\n",
    "            major = hum_ind[:upper]\n",
    "            minor = mac_ind[:lower]\n",
    "        else:\n",
    "            upper = int(lower*upper_fra) if lower * \\\n",
    "                upper_fra < len(mac_ind) else len(mac_ind)\n",
    "            major = mac_ind[:upper]\n",
    "            minor = hum_ind[:lower]\n",
    "\n",
    "        # resampling\n",
    "        add_n = upper - lower\n",
    "        oversampled = []\n",
    "        while (len(oversampled) < add_n):\n",
    "            oversampled.append(random.choice(mac_ind))\n",
    "        sel_lit = major + minor + oversampled\n",
    "        random.shuffle(sel_lit)\n",
    "\n",
    "        self.train_x = self.train_x.iloc[sel_lit]\n",
    "        self.train_y = self.train_y.iloc[sel_lit]\n",
    "\n",
    "    def test_down(self, frac=1):\n",
    "        ## down sample the majority class in test sets to have same number of record with the minority class\n",
    "        mac_ind = self.test_y[self.test_y == MACHINE_IND].index.to_list()\n",
    "        hum_ind = self.test_y[self.test_y == HUMAN_IND].index.to_list()\n",
    "        lower = min(len(mac_ind), len(hum_ind))\n",
    "        if frac > 1:\n",
    "            sel_lit = mac_ind[:lower] + hum_ind[:int(lower/frac)]\n",
    "        else:\n",
    "            sel_lit = mac_ind[:int(lower*frac)] + hum_ind[:lower]\n",
    "        self.test_x = self.test_x.iloc[sel_lit]\n",
    "        self.test_y = self.test_y.iloc[sel_lit]\n",
    "        random.shuffle(sel_lit)\n",
    "\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    # Calculate precision and recall\n",
    "    tp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)))\n",
    "    fp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)))\n",
    "    fn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)))\n",
    "    precision = tp / (tp + fp + backend.epsilon())\n",
    "    recall = tp / (tp + fn + backend.epsilon())\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1_score = 2 * precision * recall / \\\n",
    "        (precision + recall + backend.epsilon())\n",
    "\n",
    "    # Return negative F1 score as the loss (to minimize it)\n",
    "    return -f1_score\n",
    "\n",
    "\n",
    "# transformer / embedding block design/code from Bharath K\n",
    "# https://blog.paperspace.com/transformers-text-classification/\n",
    "\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, drop_rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = Sequential(\n",
    "            [Dense(ff_dim, activation=\"relu\"),\n",
    "             Dense(embed_dim),]\n",
    "        )\n",
    "        self.dropout1 = Dropout(drop_rate)\n",
    "        self.dropout2 = Dropout(drop_rate)\n",
    "        self.lay_nor_1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.lay_nor_2 = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.attention(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.lay_nor_1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.lay_nor_2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "class EmbeddingLayer(Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "random.seed(RANDOM_SEED)\n",
    "## using GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "if len(tf.config.list_physical_devices('GPU')):\n",
    "    tf.config.list_physical_devices('GPU')\n",
    "    print(\"Using GPU\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _______________ Read data from domain 1 _______________\n",
    "man_1_df = pd.read_json(HUMAN_1_P)\n",
    "man_1_df[\"label\"] = HUMAN_IND\n",
    "mac_1_df = pd.read_json(MACHINE_1_P).drop(\"machine_id\", axis=1)\n",
    "mac_1_df[\"label\"] = MACHINE_IND\n",
    "domain_1_df = pd.concat([man_1_df, mac_1_df])\n",
    "\n",
    "domain_1 = DomainData(domain_1_df[[\"prompt\", \"txt\"]], domain_1_df[\"label\"])\n",
    "domain_1.t_t_spli(TEST_FRA, RANDOM_SEED)\n",
    "domain_1.over_sampling(1.6)\n",
    "domain_1.test_down()\n",
    "domain_1.add_padding('post', MAX_PRO_LEN, MAX_TXT_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 13:43:51.939711: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-27 13:43:51.940468: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-27 13:43:51.941316: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-27 13:43:53.120370: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-27 13:43:53.121153: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-27 13:43:53.121182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-04-27 13:43:53.121639: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-27 13:43:53.121706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3383 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 13:43:57.063020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-04-27 13:43:57.106815: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f9ee00a6c00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-04-27 13:43:57.106854: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2023-04-27 13:43:57.132916: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-04-27 13:43:57.362734: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-04-27 13:43:57.570920: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 18s 212ms/step - loss: 0.3603 - accuracy: 0.8570 - f1_loss: -0.8582 - val_loss: 0.2675 - val_accuracy: 0.8890 - val_f1_loss: -0.8785\n",
      "Epoch 2/60\n",
      "56/56 [==============================] - 9s 163ms/step - loss: 0.2562 - accuracy: 0.8979 - f1_loss: -0.8939 - val_loss: 0.2122 - val_accuracy: 0.9102 - val_f1_loss: -0.9055\n",
      "Epoch 3/60\n",
      "56/56 [==============================] - 7s 125ms/step - loss: 0.1677 - accuracy: 0.9319 - f1_loss: -0.9310 - val_loss: 0.1894 - val_accuracy: 0.9325 - val_f1_loss: -0.9283\n",
      "Epoch 4/60\n",
      "56/56 [==============================] - 6s 97ms/step - loss: 0.1304 - accuracy: 0.9459 - f1_loss: -0.9458 - val_loss: 0.1968 - val_accuracy: 0.9269 - val_f1_loss: -0.9218\n",
      "Epoch 5/60\n",
      "56/56 [==============================] - 5s 88ms/step - loss: 0.0775 - accuracy: 0.9708 - f1_loss: -0.9709 - val_loss: 0.2360 - val_accuracy: 0.9252 - val_f1_loss: -0.9260\n",
      "Epoch 6/60\n",
      "56/56 [==============================] - 5s 84ms/step - loss: 0.0359 - accuracy: 0.9869 - f1_loss: -0.9868 - val_loss: 0.2971 - val_accuracy: 0.9459 - val_f1_loss: -0.9442\n",
      "Epoch 7/60\n",
      "56/56 [==============================] - 4s 78ms/step - loss: 0.0566 - accuracy: 0.9798 - f1_loss: -0.9800 - val_loss: 0.2105 - val_accuracy: 0.9431 - val_f1_loss: -0.9402\n",
      "Epoch 8/60\n",
      "56/56 [==============================] - 4s 72ms/step - loss: 0.0185 - accuracy: 0.9954 - f1_loss: -0.9954 - val_loss: 0.4158 - val_accuracy: 0.9224 - val_f1_loss: -0.9153\n",
      "Epoch 9/60\n",
      "56/56 [==============================] - 4s 64ms/step - loss: 0.0107 - accuracy: 0.9968 - f1_loss: -0.9967 - val_loss: 0.3637 - val_accuracy: 0.9436 - val_f1_loss: -0.9417\n",
      "Epoch 10/60\n",
      "56/56 [==============================] - 3s 56ms/step - loss: 0.0047 - accuracy: 0.9987 - f1_loss: -0.9987 - val_loss: 0.4291 - val_accuracy: 0.9369 - val_f1_loss: -0.9337\n",
      "Epoch 11/60\n",
      "56/56 [==============================] - 3s 60ms/step - loss: 0.0035 - accuracy: 0.9992 - f1_loss: -0.9992 - val_loss: 0.4190 - val_accuracy: 0.9414 - val_f1_loss: -0.9388\n",
      "Epoch 12/60\n",
      "56/56 [==============================] - 3s 62ms/step - loss: 0.0023 - accuracy: 0.9993 - f1_loss: -0.9993 - val_loss: 0.4551 - val_accuracy: 0.9448 - val_f1_loss: -0.9429\n",
      "Epoch 13/60\n",
      "56/56 [==============================] - 3s 56ms/step - loss: 0.0023 - accuracy: 0.9996 - f1_loss: -0.9996 - val_loss: 0.5479 - val_accuracy: 0.9336 - val_f1_loss: -0.9292\n",
      "Epoch 14/60\n",
      "56/56 [==============================] - 3s 50ms/step - loss: 0.0145 - accuracy: 0.9944 - f1_loss: -0.9943 - val_loss: 0.4668 - val_accuracy: 0.9252 - val_f1_loss: -0.9181\n",
      "Epoch 15/60\n",
      "56/56 [==============================] - 3s 62ms/step - loss: 0.0114 - accuracy: 0.9954 - f1_loss: -0.9953 - val_loss: 0.4160 - val_accuracy: 0.9448 - val_f1_loss: -0.9429\n",
      "Epoch 16/60\n",
      "56/56 [==============================] - 3s 52ms/step - loss: 0.0028 - accuracy: 0.9987 - f1_loss: -0.9988 - val_loss: 0.4565 - val_accuracy: 0.9475 - val_f1_loss: -0.9456\n",
      "Model Saved: trans_model.h5\n",
      "Model Loaded: trans_model.h5\n",
      "loss:  0.25016483664512634\n",
      "accuracy 0.897857129573822\n",
      "44/44 [==============================] - 1s 7ms/step\n",
      "[[639  61]\n",
      " [ 82 618]]\n",
      "f1-score:  0.8963016678752719\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "num_heads = 2\n",
    "ff_dim = 64\n",
    "epo_size = NO_EPO\n",
    "batch_size = 128\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "\n",
    "# Set Prompt input\n",
    "inputs_p = Input(shape=(MAX_PRO_LEN,))\n",
    "embedding_layer = EmbeddingLayer(MAX_PRO_LEN, NUM_TOKEN, embed_dim)\n",
    "x = embedding_layer(inputs_p)\n",
    "x = transformer_block(x)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "\n",
    "# Set txt input\n",
    "inputs_t = Input(shape=(MAX_TXT_LEN,))\n",
    "embedding_layer = EmbeddingLayer(MAX_TXT_LEN, NUM_TOKEN, embed_dim)\n",
    "y = embedding_layer(inputs_t)\n",
    "y = transformer_block(y)\n",
    "y = GlobalAveragePooling1D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "y = Dense(64, activation=\"relu\")(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "\n",
    "# Concatenate outputs from prompt and text models\n",
    "merged = Concatenate()([x, y])\n",
    "merged = Dense(units=64, activation='relu')(merged)\n",
    "merged = Dense(32, activation=\"relu\")(merged)\n",
    "outputs = Dense(units=1, activation='sigmoid')(merged)\n",
    "trans_model_2 = Model(inputs=[inputs_p, inputs_t], outputs=outputs)\n",
    "\n",
    "# Compile and train\n",
    "trans_model_2.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\n",
    "                      \"accuracy\", f1_loss])\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'trans_model.h5', monitor='val_loss', save_best_only=True)\n",
    "trans_model_2.fit([domain_1.train_prompt, domain_1.train_txt], domain_1.train_label, epochs=epo_size,\n",
    "                  batch_size=batch_size, validation_split=0.2, callbacks=[callback, model_checkpoint])\n",
    "print(\"Model Saved: trans_model.h5\")\n",
    "\n",
    "# evaluate\n",
    "trans_model_2 = tf.keras.models.load_model(\"trans_model.h5\", custom_objects={\n",
    "                                           'f1_loss': f1_loss, 'TransformerBlock': TransformerBlock, 'EmbeddingLayer': EmbeddingLayer})\n",
    "print(\"Model Loaded: trans_model.h5\")\n",
    "loss, accuracy, f1 = trans_model_2.evaluate(\n",
    "    [domain_1.test_prompt, domain_1.test_txt], domain_1.test_label, verbose=False)\n",
    "print(\"loss: \", loss)\n",
    "print(\"accuracy\", accuracy)\n",
    "trans_1_pre_rnn = trans_model_2.predict(\n",
    "    [domain_1.test_prompt, domain_1.test_txt])\n",
    "trans_1_pre_rnn = np.round(trans_1_pre_rnn).flatten()\n",
    "confusion = confusion_matrix(domain_1.test_label, trans_1_pre_rnn)\n",
    "# trans_1_pre_rnn = [0 if i.flatten()[0] > i.flatten()[1] else 1 for i in trans_1_pre_rnn]\n",
    "# confusion = confusion_matrix(domain_1.test_label, trans_1_pre_rnn)\n",
    "print(confusion)\n",
    "f1 = f1_score(domain_1.test_label, trans_1_pre_rnn)\n",
    "print(\"f1-score: \", f1)\n",
    "\n",
    "# 609/652 dropout -> 0.3 || 645/610\n",
    "# 645/610 dense 20 -> 32 || 642/624\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain 2 weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_fra = 1.6\n",
    "weight_fra = 300\n",
    "\n",
    "# _______________ Read data from domain 1 _______________\n",
    "man_1_df = pd.read_json(HUMAN_1_P)\n",
    "man_1_df[\"label\"] = HUMAN_IND\n",
    "mac_1_df = pd.read_json(MACHINE_1_P).drop(\"machine_id\", axis=1)\n",
    "mac_1_df[\"label\"] = MACHINE_IND\n",
    "domain_1_df = pd.concat([man_1_df, mac_1_df])\n",
    "\n",
    "domain_1 = DomainData(domain_1_df[[\"prompt\", \"txt\"]], domain_1_df[\"label\"])\n",
    "domain_1.t_t_spli(TEST_FRA, RANDOM_SEED)\n",
    "# domain_1.down_sampling()\n",
    "domain_1.add_padding('post', MAX_PRO_LEN, MAX_TXT_LEN)\n",
    "\n",
    "# _______________ Read data from domain 2 _______________\n",
    "man_2_df = pd.read_json(HUMAN_2_P)\n",
    "man_2_df[\"label\"] = HUMAN_IND\n",
    "mac_2_df = pd.read_json(MACHINE_2_P).drop(\"machine_id\", axis=1)\n",
    "mac_2_df[\"label\"] = MACHINE_IND\n",
    "domain_2_df = pd.concat([man_2_df, mac_2_df])\n",
    "\n",
    "domain_2 = DomainData(domain_2_df[[\"prompt\", \"txt\"]], domain_2_df[\"label\"])\n",
    "domain_2.t_t_spli(TEST_FRA, RANDOM_SEED)\n",
    "# domain_2.over_sampling(over_fra)\n",
    "domain_2.test_down()\n",
    "domain_2.add_padding('post', MAX_PRO_LEN, MAX_TXT_LEN)\n",
    "\n",
    "\n",
    "# _______________ weight data _______________\n",
    "sample_weight_1 = np.ones(len(domain_1.train_label))\n",
    "sample_weight_2 = np.ones(len(domain_2.train_label))\n",
    "sample_weight_2 *= weight_fra\n",
    "sample_weight = np.concatenate([sample_weight_1, sample_weight_2])\n",
    "\n",
    "train_prompt = np.concatenate([domain_1.train_prompt, domain_2.train_prompt])\n",
    "train_txt = np.concatenate([domain_1.train_txt, domain_2.train_txt])\n",
    "train_label = np.concatenate([domain_1.train_label, domain_2.train_label])\n",
    "\n",
    "data = list(zip(train_prompt, train_txt, train_label, sample_weight))\n",
    "random.shuffle(data)\n",
    "\n",
    "train_prompt, train_txt, train_label, sample_weight = zip(*data)\n",
    "train_prompt = np.array(train_prompt)\n",
    "train_txt = np.array(train_txt)\n",
    "train_label = np.array(train_label)\n",
    "sample_weight = np.array(sample_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "633/633 [==============================] - ETA: 0s - loss: 0.9263 - accuracy: 0.9296 - f1_loss: -0.9593WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "633/633 [==============================] - 108s 165ms/step - loss: 0.9263 - accuracy: 0.9296 - f1_loss: -0.9593 - val_loss: 0.5954 - val_accuracy: 0.9746 - val_f1_loss: -0.9870\n",
      "Epoch 2/10\n",
      "633/633 [==============================] - ETA: 0s - loss: 0.5982 - accuracy: 0.9635 - f1_loss: -0.9808WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "633/633 [==============================] - 82s 129ms/step - loss: 0.5982 - accuracy: 0.9635 - f1_loss: -0.9808 - val_loss: 0.5809 - val_accuracy: 0.9740 - val_f1_loss: -0.9866\n",
      "Epoch 3/10\n",
      "633/633 [==============================] - ETA: 0s - loss: 0.2904 - accuracy: 0.9715 - f1_loss: -0.9853WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "633/633 [==============================] - 87s 137ms/step - loss: 0.2904 - accuracy: 0.9715 - f1_loss: -0.9853 - val_loss: 0.4376 - val_accuracy: 0.9755 - val_f1_loss: -0.9875\n",
      "Epoch 4/10\n",
      "633/633 [==============================] - ETA: 0s - loss: 0.1515 - accuracy: 0.9756 - f1_loss: -0.9873WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "633/633 [==============================] - 82s 129ms/step - loss: 0.1515 - accuracy: 0.9756 - f1_loss: -0.9873 - val_loss: 0.5407 - val_accuracy: 0.9749 - val_f1_loss: -0.9872\n",
      "Epoch 5/10\n",
      "633/633 [==============================] - ETA: 0s - loss: 0.1108 - accuracy: 0.9785 - f1_loss: -0.9889WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "633/633 [==============================] - 80s 126ms/step - loss: 0.1108 - accuracy: 0.9785 - f1_loss: -0.9889 - val_loss: 0.4353 - val_accuracy: 0.9823 - val_f1_loss: -0.9909\n",
      "Epoch 6/10\n",
      "633/633 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9814 - f1_loss: -0.9901WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "633/633 [==============================] - 81s 127ms/step - loss: 0.0666 - accuracy: 0.9814 - f1_loss: -0.9901 - val_loss: 1.3238 - val_accuracy: 0.9773 - val_f1_loss: -0.9884\n",
      "Epoch 7/10\n",
      "633/633 [==============================] - ETA: 0s - loss: 0.1550 - accuracy: 0.9773 - f1_loss: -0.9878WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "633/633 [==============================] - 80s 126ms/step - loss: 0.1550 - accuracy: 0.9773 - f1_loss: -0.9878 - val_loss: 0.4073 - val_accuracy: 0.9818 - val_f1_loss: -0.9907\n",
      "Epoch 8/10\n",
      "633/633 [==============================] - ETA: 0s - loss: 0.0517 - accuracy: 0.9853 - f1_loss: -0.9924WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "633/633 [==============================] - 80s 127ms/step - loss: 0.0517 - accuracy: 0.9853 - f1_loss: -0.9924 - val_loss: 0.4214 - val_accuracy: 0.9825 - val_f1_loss: -0.9910\n",
      "Epoch 9/10\n",
      "633/633 [==============================] - ETA: 0s - loss: 0.0409 - accuracy: 0.9878 - f1_loss: -0.9937WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "633/633 [==============================] - 80s 126ms/step - loss: 0.0409 - accuracy: 0.9878 - f1_loss: -0.9937 - val_loss: 0.4486 - val_accuracy: 0.9837 - val_f1_loss: -0.9917\n",
      "Epoch 10/10\n",
      "633/633 [==============================] - ETA: 0s - loss: 0.0346 - accuracy: 0.9895 - f1_loss: -0.9946WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "633/633 [==============================] - 80s 126ms/step - loss: 0.0346 - accuracy: 0.9895 - f1_loss: -0.9946 - val_loss: 0.5168 - val_accuracy: 0.9845 - val_f1_loss: -0.9921\n",
      "Model Saved: trans_model_weighted.h5\n",
      "Model Loaded: trans_model_weighted.h5\n",
      "loss:  0.1168220266699791\n",
      "accuracy 0.949999988079071\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "[[20  0]\n",
      " [ 2 18]]\n",
      "f1-score:  0.9473684210526316\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "num_heads = 10\n",
    "ff_dim = 32\n",
    "epo_size = 10\n",
    "batch_size = 128\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "\n",
    "# Set Prompt input\n",
    "inputs_p = Input(shape=(MAX_PRO_LEN,))\n",
    "embedding_layer = EmbeddingLayer(MAX_PRO_LEN, NUM_TOKEN, embed_dim)\n",
    "x = embedding_layer(inputs_p)\n",
    "x = transformer_block(x)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(20, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "\n",
    "# Set txt input\n",
    "inputs_t = Input(shape=(MAX_TXT_LEN,))\n",
    "embedding_layer = EmbeddingLayer(MAX_TXT_LEN, NUM_TOKEN, embed_dim)\n",
    "y = embedding_layer(inputs_t)\n",
    "y = transformer_block(y)\n",
    "y = GlobalAveragePooling1D()(y)\n",
    "y = Dropout(0.1)(y)\n",
    "y = Dense(20, activation=\"relu\")(y)\n",
    "y = Dropout(0.1)(y)\n",
    "\n",
    "\n",
    "# Concatenate outputs from prompt and text models\n",
    "merged = Concatenate()([x, y])\n",
    "merged = Dense(units=64, activation='relu')(merged)\n",
    "merged = Dense(20, activation=\"relu\")(merged)\n",
    "outputs = Dense(units=1, activation='sigmoid')(merged)\n",
    "trans_model_2 = Model(inputs=[inputs_p, inputs_t], outputs=outputs)\n",
    "\n",
    "# Compile and train\n",
    "trans_model_2.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\n",
    "                      \"accuracy\", f1_loss])\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'trans_model_weighted.h5', monitor='val_loss', save_best_only=True)\n",
    "trans_model_2.fit([train_prompt, train_txt], train_label, epochs=epo_size, batch_size=batch_size,\n",
    "                  sample_weight=sample_weight, validation_split=0.2, callbacks=[callback, model_checkpoint])\n",
    "print(\"Model Saved: trans_model_weighted.h5\")\n",
    "\n",
    "\n",
    "## evaluate\n",
    "trans_model_2 = tf.keras.models.load_model(\"trans_model_weighted.h5\", custom_objects={\n",
    "                                           'f1_loss': f1_loss, 'TransformerBlock': TransformerBlock, 'EmbeddingLayer': EmbeddingLayer})\n",
    "print(\"Model Loaded: trans_model_weighted.h5\")\n",
    "loss, accuracy, f1 = trans_model_2.evaluate(\n",
    "    [domain_2.test_prompt, domain_2.test_txt], domain_2.test_label, verbose=False)\n",
    "print(\"loss: \", loss)\n",
    "print(\"accuracy\", accuracy)\n",
    "trans_2_pre_rnn = trans_model_2.predict(\n",
    "    [domain_2.test_prompt, domain_2.test_txt])\n",
    "trans_2_pre_rnn = np.round(trans_2_pre_rnn).flatten()\n",
    "confusion = confusion_matrix(domain_2.test_label, trans_2_pre_rnn)\n",
    "print(confusion)\n",
    "f1 = f1_score(domain_2.test_label, trans_2_pre_rnn)\n",
    "print(\"f1-score: \", f1)\n",
    "\n",
    "# 20/18 epoch: NO_EPO -> 10 ||  20/17\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 11ms/step\n",
      "13/13 [==============================] - 0s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "DOMAIN_SPL = 600\n",
    "## prepare data\n",
    "test_df = pd.read_json(TEST_P)\n",
    "test_prompt = pad_sequences(\n",
    "    test_df[\"prompt\"], padding=\"post\", maxlen=MAX_PRO_LEN)\n",
    "test_txt = pad_sequences(test_df[\"txt\"], padding=\"post\", maxlen=MAX_TXT_LEN)\n",
    "\n",
    "## train model\n",
    "model_1 = tf.keras.models.load_model(\"trans_model.h5\", custom_objects={\n",
    "                                     'f1_loss': f1_loss, 'TransformerBlock': TransformerBlock, 'EmbeddingLayer': EmbeddingLayer})\n",
    "model_2 = tf.keras.models.load_model(\"trans_model_weighted.h5\", custom_objects={\n",
    "                                     'f1_loss': f1_loss, 'TransformerBlock': TransformerBlock, 'EmbeddingLayer': EmbeddingLayer})\n",
    "\n",
    "## predict\n",
    "pred = []\n",
    "pred += model_1.predict([test_prompt[:DOMAIN_SPL],\n",
    "                        test_txt[:DOMAIN_SPL]]).tolist()\n",
    "pred += model_2.predict([test_prompt[DOMAIN_SPL:],\n",
    "                        test_txt[DOMAIN_SPL:]]).tolist()\n",
    "pred = [int(i) for i in np.round(pred).flatten()]\n",
    "\n",
    "\n",
    "## save\n",
    "pred_df = pd.DataFrame(pred)\n",
    "pred_df.columns = [\"Predicted\"]\n",
    "pred_df.index.names = ['Id']\n",
    "pred_df.to_csv(\"./data/result3.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
