{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OVERSAMPLING + DOWNSAMPLING in DomainData.over_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 14:37:12.300227: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-19 14:37:12.376425: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-19 14:37:13.379043: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Using GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 14:37:15.184966: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-19 14:37:15.219455: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-19 14:37:15.219996: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import backend\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "from keras import layers\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "NUM_TOKEN = 5000\n",
    "MAX_PRO_LEN = 64\n",
    "MAX_TXT_LEN = 256\n",
    "NO_EPO = 4\n",
    "NO_BAT = 128\n",
    "\n",
    "MACHINE_1_P = \"./data/set1_machine.json\"\n",
    "HUMAN_1_P = \"./data/set1_human.json\"\n",
    "MACHINE_2_P = \"./data/set2_machine.json\"\n",
    "HUMAN_2_P = \"./data/set2_human.json\"\n",
    "TEST_P = \"./data/test.json\"\n",
    "RANDOM_SEED = 42\n",
    "MACHINE_IND = 0\n",
    "HUMAN_IND = 1\n",
    "TEST_FRA = 0.2\n",
    "\n",
    "class DomainData:\n",
    "    \"\"\"\n",
    "    train_test_split, pad_sequence, PCA, class_weight\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def t_t_spli(self, test_size, random_state):\n",
    "        self.random_state = random_state\n",
    "        self.train_x, self.test_x, self.train_y, self.test_y = train_test_split(self.x, self.y, test_size=test_size, stratify = self.y, random_state = random_state)\n",
    "        self.train_x = self.train_x.reset_index(drop=True)\n",
    "        self.train_y = self.train_y.reset_index(drop=True)\n",
    "        self.test_x = self.test_x.reset_index(drop=True)\n",
    "        self.test_y = self.test_y.reset_index(drop=True)\n",
    "        \n",
    "\n",
    "    def add_padding(self, padding, prompt_len, txt_len):\n",
    "        self.train_prompt = self.train_x[\"prompt\"]\n",
    "        self.train_txt = self.train_x[\"txt\"]\n",
    "        self.train_label = self.train_y.to_numpy()\n",
    "        self.test_prompt = self.test_x[\"prompt\"]\n",
    "        self.test_txt = self.test_x[\"txt\"]\n",
    "        self.test_label = self.test_y.to_numpy()\n",
    "        unique_classes = np.unique(self.train_label)\n",
    "        class_weights = class_weight.compute_class_weight(\"balanced\", classes=unique_classes, y=self.train_y)\n",
    "        self.class_weights = dict(zip(unique_classes, class_weights))\n",
    "        \n",
    "        self.prompt_len = prompt_len\n",
    "        self.txt_len = txt_len\n",
    "        \n",
    "        self.train_prompt = pad_sequences(self.train_prompt, padding=padding, maxlen=prompt_len)\n",
    "        self.train_txt = pad_sequences(self.train_txt, padding=padding, maxlen=txt_len)\n",
    "        self.test_prompt = pad_sequences(self.test_prompt, padding=padding, maxlen=prompt_len)\n",
    "        self.test_txt = pad_sequences(self.test_txt, padding=padding, maxlen=txt_len)\n",
    "        \n",
    "        \n",
    "    def down_sampling(self):\n",
    "        mac_ind = self.train_y[self.train_y == MACHINE_IND].index.to_list()\n",
    "        hum_ind = self.train_y[self.train_y == HUMAN_IND].index.to_list()\n",
    "        lower = min(len(mac_ind), len(hum_ind))\n",
    "        sel_lit = mac_ind[:lower] + hum_ind[:lower]\n",
    "        self.train_x = self.train_x.iloc[sel_lit]\n",
    "        self.train_y = self.train_y.iloc[sel_lit]\n",
    "        random.shuffle(sel_lit)\n",
    "\n",
    "    def over_sampling(self, upper_fra):\n",
    "        ## ______________________ oversampling + downsampling ______________________\n",
    "        mac_ind = self.train_y[self.train_y == MACHINE_IND].index.to_list()\n",
    "        hum_ind = self.train_y[self.train_y == HUMAN_IND].index.to_list()\n",
    "        lower = min(len(mac_ind), len(hum_ind))\n",
    "        if lower == len(mac_ind):\n",
    "            upper = int(lower*upper_fra) if lower*upper_fra < len(hum_ind) else len(hum_ind)\n",
    "            major = hum_ind[:upper]\n",
    "            minor = mac_ind[:lower]\n",
    "    \n",
    "        else:\n",
    "            upper = int(lower*upper_fra) if lower*upper_fra < len(mac_ind) else len(mac_ind)\n",
    "            major = mac_ind[:upper]\n",
    "            minor = hum_ind[:lower]\n",
    "        \n",
    "        add_n = upper - lower\n",
    "        oversampled = []\n",
    "        while(len(oversampled) < add_n):\n",
    "            oversampled.append(random.choice(mac_ind))\n",
    "        sel_lit = major + minor + oversampled\n",
    "        random.shuffle(sel_lit)\n",
    "        \n",
    "        self.train_x = self.train_x.iloc[sel_lit]\n",
    "        self.train_y = self.train_y.iloc[sel_lit]\n",
    "    \n",
    "    def test_down(self, frac = 1):\n",
    "        mac_ind = self.test_y[self.test_y == MACHINE_IND].index.to_list()\n",
    "        hum_ind = self.test_y[self.test_y == HUMAN_IND].index.to_list()\n",
    "        lower = min(len(mac_ind), len(hum_ind))\n",
    "        if frac > 1:\n",
    "            sel_lit = mac_ind[:lower] + hum_ind[:int(lower/frac)]\n",
    "        else:\n",
    "            sel_lit = mac_ind[:int(lower*frac)] + hum_ind[:lower]\n",
    "        self.test_x = self.test_x.iloc[sel_lit]\n",
    "        self.test_y = self.test_y.iloc[sel_lit]\n",
    "        random.shuffle(sel_lit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    # Calculate precision and recall\n",
    "    tp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)))\n",
    "    fp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)))\n",
    "    fn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)))\n",
    "    precision = tp / (tp + fp + backend.epsilon())\n",
    "    recall = tp / (tp + fn + backend.epsilon())\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1_score = 2 * precision * recall / (precision + recall + backend.epsilon())\n",
    "    \n",
    "    # Return negative F1 score as the loss (to minimize it)\n",
    "    return -f1_score\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience = 3)\n",
    "random.seed(RANDOM_SEED)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "if len(tf.config.list_physical_devices('GPU')):\n",
    "    tf.config.list_physical_devices('GPU')\n",
    "    print(\"Using GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# ## _______________ Plot distribution _______________\n",
    "# man_1_df = pd.read_json(HUMAN_1_P)\n",
    "# man_1_df[\"label\"] = HUMAN_IND\n",
    "# mac_1_df = pd.read_json(MACHINE_1_P).drop(\"machine_id\", axis = 1)\n",
    "# mac_1_df[\"label\"] = MACHINE_IND\n",
    "# domain_1_df = pd.concat([man_1_df, mac_1_df])\n",
    "\n",
    "# domain_1_df[\"length_p\"] = domain_1_df[\"prompt\"].apply(lambda x: len(x))\n",
    "# domain_1_df[\"length_t\"] = domain_1_df[\"txt\"].apply(lambda x: len(x))\n",
    "\n",
    "# domain_1_df[\"length_t\"].plot.hist()\n",
    "# plt.xlim(100, 1000)\n",
    "# plt.show()\n",
    "\n",
    "# print(domain_1_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## _______________ Read data from domain 1 _______________\n",
    "man_1_df = pd.read_json(HUMAN_1_P)\n",
    "man_1_df[\"label\"] = HUMAN_IND\n",
    "mac_1_df = pd.read_json(MACHINE_1_P).drop(\"machine_id\", axis = 1)\n",
    "mac_1_df[\"label\"] = MACHINE_IND\n",
    "domain_1_df = pd.concat([man_1_df, mac_1_df])\n",
    "\n",
    "domain_1 = DomainData(domain_1_df[[\"prompt\", \"txt\"]], domain_1_df[\"label\"])\n",
    "domain_1.t_t_spli(TEST_FRA, RANDOM_SEED)\n",
    "domain_1.over_sampling(1.6)\n",
    "domain_1.test_down()\n",
    "domain_1.add_padding('post', MAX_PRO_LEN, MAX_TXT_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 14:38:39.703390: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-04-19 14:38:39.932744: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-04-19 14:38:42.074962: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-04-19 14:38:42.314273: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 21s 248ms/step - loss: 0.5900 - accuracy: 0.6882 - f1_loss: -0.6805 - val_loss: 0.3712 - val_accuracy: 0.8276 - val_f1_loss: -0.8302\n",
      "Epoch 2/4\n",
      "56/56 [==============================] - 10s 181ms/step - loss: 0.2089 - accuracy: 0.9192 - f1_loss: -0.9195 - val_loss: 0.2019 - val_accuracy: 0.9202 - val_f1_loss: -0.9158\n",
      "Epoch 3/4\n",
      "56/56 [==============================] - 8s 145ms/step - loss: 0.0587 - accuracy: 0.9798 - f1_loss: -0.9797 - val_loss: 0.2094 - val_accuracy: 0.9230 - val_f1_loss: -0.9161\n",
      "Epoch 4/4\n",
      "56/56 [==============================] - 7s 117ms/step - loss: 0.0155 - accuracy: 0.9962 - f1_loss: -0.9963 - val_loss: 0.2650 - val_accuracy: 0.9230 - val_f1_loss: -0.9215\n",
      "loss:  0.41972485184669495\n",
      "accuracy 0.8785714507102966\n",
      "44/44 [==============================] - 3s 23ms/step\n",
      "[[579 121]\n",
      " [ 49 651]]\n",
      "f1-score:  0.8845108695652174\n"
     ]
    }
   ],
   "source": [
    "# best: 633  /642: epo->4. bat->128\n",
    "epo_size = 4\n",
    "batch_size = 128\n",
    "\n",
    "## _______________ RNN on Prompts _______________\n",
    "p_model_rnn= tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=NUM_TOKEN,\n",
    "        output_dim=128,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    # tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5)\n",
    "    # tf.keras.layers.LSTM(32)\n",
    "])\n",
    "\n",
    "## _______________ RNN on txt _______________\n",
    "t_model_rnn= tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=NUM_TOKEN,\n",
    "        output_dim=128,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    # tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5)\n",
    "    # tf.keras.layers.LSTM(64)\n",
    "])\n",
    "\n",
    "## _______________ Combined _______________\n",
    "prompt_input = layers.Input(shape=(MAX_PRO_LEN,))\n",
    "text_input = layers.Input(shape=(MAX_TXT_LEN,))\n",
    "prompt_output = p_model_rnn(prompt_input)\n",
    "text_output = t_model_rnn(text_input)\n",
    "merged = layers.concatenate([prompt_output, text_output])\n",
    "merged = layers.Dense(64, activation='tanh')(merged)\n",
    "merged = layers.Dropout(0.5)(merged)\n",
    "merged = layers.Dense(1, activation='sigmoid')(merged)\n",
    "combined_model = tf.keras.models.Model(inputs=[prompt_input, text_input], outputs=merged)\n",
    "combined_model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy', f1_loss])\n",
    "\n",
    "# combined_model.fit([domain_1.train_prompt, domain_1.train_txt], domain_1.train_label, epochs=epo_size, batch_size=batch_size, validation_split=0.2, class_weight=domain_1.class_weights, callbacks = [callback])\n",
    "combined_model.fit([domain_1.train_prompt, domain_1.train_txt], domain_1.train_label, epochs=epo_size, batch_size=batch_size, validation_split=0.2, callbacks = [callback])\n",
    "combined_model.save(\"combined_model.h5\")\n",
    "\n",
    "## evaluate\n",
    "loss, accuracy, f1 = combined_model.evaluate([domain_1.test_prompt, domain_1.test_txt], domain_1.test_label, verbose=False)\n",
    "print(\"loss: \", loss)\n",
    "print(\"accuracy\", accuracy)\n",
    "comb_1_pre_rnn = combined_model.predict([domain_1.test_prompt, domain_1.test_txt])\n",
    "comb_1_pre_rnn = np.round(comb_1_pre_rnn).flatten()\n",
    "confusion = confusion_matrix(domain_1.test_label, comb_1_pre_rnn)\n",
    "print(confusion)\n",
    "f1 = f1_score(domain_1.test_label, comb_1_pre_rnn)\n",
    "print(\"f1-score: \", f1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain 2: Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## _______________ Read data from domain 2 _______________\n",
    "man_2_df = pd.read_json(HUMAN_2_P)\n",
    "man_2_df[\"label\"] = HUMAN_IND\n",
    "mac_2_df = pd.read_json(MACHINE_2_P).drop(\"machine_id\", axis = 1)\n",
    "mac_2_df[\"label\"] = MACHINE_IND\n",
    "domain_2_df = pd.concat([man_2_df, mac_2_df])\n",
    "\n",
    "domain_2 = DomainData(domain_2_df[[\"prompt\", \"txt\"]], domain_2_df[\"label\"])\n",
    "domain_2.t_t_spli(TEST_FRA, RANDOM_SEED)\n",
    "domain_2.add_padding('post', MAX_PRO_LEN, MAX_TXT_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6710 - accuracy: 0.8150 - f1_loss: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "epo_size = NO_EPO\n",
    "batch_size = 40\n",
    "\n",
    "\n",
    "prompt_input = layers.Input(shape=(MAX_PRO_LEN,))\n",
    "text_input = layers.Input(shape=(MAX_TXT_LEN,))\n",
    "\n",
    "## _______________ Load model and continue training _______________\n",
    "combined_model_2 = tf.keras.models.load_model(\"combined_model.h5\", custom_objects={ 'f1_loss': f1_loss })\n",
    "combined_model_2.layers.pop()\n",
    "new_layer = layers.Dense(1, activation='sigmoid')(combined_model_2.output)\n",
    "combined_model_2 = tf.keras.models.Model(inputs=combined_model_2.input, outputs=new_layer)\n",
    "combined_model_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', f1_loss])\n",
    "\n",
    "combined_model_2.fit([domain_2.train_prompt, domain_2.train_txt], domain_2.train_label, epochs=epo_size, batch_size=batch_size, validation_split=0.2, class_weight=domain_2.class_weights, callbacks = [callback])\n",
    "combined_model_2.save(\"combined_model_2.h5\")\n",
    "\n",
    "## evaluate\n",
    "loss, accuracy, f1 = combined_model_2.evaluate([domain_2.test_prompt, domain_2.test_txt], domain_2.test_label, verbose=False)\n",
    "print(\"loss: \", loss)\n",
    "print(\"accuracy\", accuracy)\n",
    "comb_2_pre_rnn = combined_model_2.predict([domain_2.test_prompt, domain_2.test_txt])\n",
    "comb_2_pre_rnn = np.round(comb_2_pre_rnn).flatten()\n",
    "confusion = confusion_matrix(domain_2.test_label, comb_2_pre_rnn)\n",
    "print(confusion)\n",
    "f1 = f1_score(domain_2.test_label, comb_2_pre_rnn)\n",
    "print(\"f1-score: \", f1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain 2: Weighted Domain Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_fra = 1.6\n",
    "weight_fra = 300\n",
    "\n",
    "## _______________ Read data from domain 1 _______________\n",
    "man_1_df = pd.read_json(HUMAN_1_P)\n",
    "man_1_df[\"label\"] = HUMAN_IND\n",
    "mac_1_df = pd.read_json(MACHINE_1_P).drop(\"machine_id\", axis = 1)\n",
    "mac_1_df[\"label\"] = MACHINE_IND\n",
    "domain_1_df = pd.concat([man_1_df, mac_1_df])\n",
    "\n",
    "domain_1 = DomainData(domain_1_df[[\"prompt\", \"txt\"]], domain_1_df[\"label\"])\n",
    "domain_1.t_t_spli(TEST_FRA, RANDOM_SEED)\n",
    "# domain_1.down_sampling()\n",
    "domain_1.add_padding('post', MAX_PRO_LEN, MAX_TXT_LEN)\n",
    "\n",
    "## _______________ Read data from domain 2 _______________\n",
    "man_2_df = pd.read_json(HUMAN_2_P)\n",
    "man_2_df[\"label\"] = HUMAN_IND\n",
    "mac_2_df = pd.read_json(MACHINE_2_P).drop(\"machine_id\", axis = 1)\n",
    "mac_2_df[\"label\"] = MACHINE_IND\n",
    "domain_2_df = pd.concat([man_2_df, mac_2_df])\n",
    "\n",
    "domain_2 = DomainData(domain_2_df[[\"prompt\", \"txt\"]], domain_2_df[\"label\"])\n",
    "domain_2.t_t_spli(TEST_FRA, RANDOM_SEED)\n",
    "# domain_2.over_sampling(over_fra)\n",
    "domain_2.test_down()\n",
    "domain_2.add_padding('post', MAX_PRO_LEN, MAX_TXT_LEN)\n",
    "\n",
    "\n",
    "\n",
    "## _______________ weight data _______________\n",
    "sample_weight_1 = np.ones(len(domain_1.train_label))\n",
    "sample_weight_2 = np.ones(len(domain_2.train_label))\n",
    "sample_weight_2 *= weight_fra\n",
    "sample_weight = np.concatenate([sample_weight_1, sample_weight_2])\n",
    "\n",
    "train_prompt = np.concatenate([domain_1.train_prompt, domain_2.train_prompt])\n",
    "train_txt = np.concatenate([domain_1.train_txt, domain_2.train_txt])\n",
    "train_label = np.concatenate([domain_1.train_label, domain_2.train_label])\n",
    "\n",
    "data = list(zip(train_prompt, train_txt, train_label, sample_weight))\n",
    "random.shuffle(data)\n",
    "\n",
    "train_prompt, train_txt, train_label, sample_weight = zip(*data)\n",
    "train_prompt = np.array(train_prompt)\n",
    "train_txt = np.array(train_txt)\n",
    "train_label = np.array(train_label)\n",
    "sample_weight = np.array(sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 12:55:00.200953: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-19 12:55:00.201510: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-19 12:55:00.202118: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-19 12:55:01.198719: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-19 12:55:01.199246: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-19 12:55:01.199264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-04-19 12:55:01.199635: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-19 12:55:01.199921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3383 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 12:55:14.966763: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-04-19 12:55:15.319235: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-04-19 12:55:18.468233: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-04-19 12:55:18.834748: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-04-19 12:55:19.927535: W tensorflow/core/common_runtime/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      " is neither a subtype nor a supertype of the combined inputs preceding it:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\twhile inferring type of node 'cond_40/output/_23'\n",
      "2023-04-19 12:55:21.878901: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-04-19 12:55:22.140323: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-04-19 12:55:22.158257: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f1b3402e1c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-04-19 12:55:22.158304: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2023-04-19 12:55:22.164694: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-04-19 12:55:22.311216: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396/396 [==============================] - ETA: 0s - loss: 1.1827 - accuracy: 0.9685 - f1_loss: -0.9837WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "396/396 [==============================] - 66s 137ms/step - loss: 1.1827 - accuracy: 0.9685 - f1_loss: -0.9837 - val_loss: 1.3052 - val_accuracy: 0.9715 - val_f1_loss: -0.9855\n",
      "Epoch 2/4\n",
      "396/396 [==============================] - ETA: 0s - loss: 0.7947 - accuracy: 0.9712 - f1_loss: -0.9853WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "396/396 [==============================] - 32s 82ms/step - loss: 0.7947 - accuracy: 0.9712 - f1_loss: -0.9853 - val_loss: 0.9133 - val_accuracy: 0.9721 - val_f1_loss: -0.9858\n",
      "Epoch 3/4\n",
      "396/396 [==============================] - ETA: 0s - loss: 0.1421 - accuracy: 0.9731 - f1_loss: -0.9863WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "396/396 [==============================] - 30s 76ms/step - loss: 0.1421 - accuracy: 0.9731 - f1_loss: -0.9863 - val_loss: 0.7067 - val_accuracy: 0.9745 - val_f1_loss: -0.9870\n",
      "Epoch 4/4\n",
      "396/396 [==============================] - ETA: 0s - loss: 0.2143 - accuracy: 0.9771 - f1_loss: -0.9883WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "396/396 [==============================] - 30s 77ms/step - loss: 0.2143 - accuracy: 0.9771 - f1_loss: -0.9883 - val_loss: 0.6982 - val_accuracy: 0.9731 - val_f1_loss: -0.9862\n",
      "loss:  0.2955428957939148\n",
      "accuracy 0.8899999856948853\n",
      "4/4 [==============================] - 3s 49ms/step\n",
      "[[49  1]\n",
      " [ 9 41]]\n",
      "f1-score:  0.8913043478260869\n"
     ]
    }
   ],
   "source": [
    "## _______________ RNN on Prompts _______________\n",
    "p_model_rnn= tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=NUM_TOKEN,\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    layers.Dropout(rate=0.5),\n",
    "    tf.keras.layers.Dense(64, activation='relu')\n",
    "])\n",
    "\n",
    "## _______________ RNN on txt _______________\n",
    "t_model_rnn= tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=NUM_TOKEN,\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    layers.Dropout(rate=0.5),\n",
    "    tf.keras.layers.Dense(64, activation='relu')\n",
    "])\n",
    "\n",
    "## _______________ Combined _______________\n",
    "prompt_input = layers.Input(shape=(MAX_PRO_LEN,))\n",
    "text_input = layers.Input(shape=(MAX_TXT_LEN,))\n",
    "prompt_output = p_model_rnn(prompt_input)\n",
    "text_output = t_model_rnn(text_input)\n",
    "merged = layers.concatenate([prompt_output, text_output])\n",
    "merged = layers.Dense(64, activation='relu')(merged)\n",
    "merged = layers.Dropout(0.5)(merged)\n",
    "merged = layers.Dense(1, activation='sigmoid')(merged)\n",
    "combined_model = tf.keras.models.Model(inputs=[prompt_input, text_input], outputs=merged)\n",
    "combined_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', f1_loss])\n",
    "\n",
    "combined_model.fit([train_prompt, train_txt], train_label, epochs=NO_EPO, batch_size=NO_BAT, validation_split=0.2, class_weight=domain_2.class_weights, sample_weight = sample_weight, callbacks = [callback])\n",
    "combined_model.save(\"combined_model_weight.h5\")\n",
    "\n",
    "## evaluate\n",
    "loss, accuracy, f1 = combined_model.evaluate([domain_2.test_prompt, domain_2.test_txt], domain_2.test_label, verbose=False)\n",
    "print(\"loss: \", loss)\n",
    "print(\"accuracy\", accuracy)\n",
    "comb_2_pre_rnn = combined_model.predict([domain_2.test_prompt, domain_2.test_txt])\n",
    "comb_2_pre_rnn = np.round(comb_2_pre_rnn).flatten()\n",
    "confusion = confusion_matrix(domain_2.test_label, comb_2_pre_rnn)\n",
    "print(confusion)\n",
    "f1 = f1_score(domain_2.test_label, comb_2_pre_rnn)\n",
    "print(\"f1-score: \", f1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2109 2263   86 ...    0    0    0]\n",
      " [2267 1890 3460 ...    0    0    0]\n",
      " [1602 2770 4021 ...    0    0    0]\n",
      " ...\n",
      " [  43 1661   46 ...   49   49   17]\n",
      " [1737   36 2607 ...    0    0    0]\n",
      " [  44   40   47 ... 3316 4183   17]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 14:03:08.892995: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-19 14:03:08.893706: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-19 14:03:08.894128: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-19 14:03:10.119621: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-19 14:03:10.120453: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-19 14:03:10.120467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-04-19 14:03:10.121212: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-19 14:03:10.121244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3383 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2023-04-19 14:03:23.123648: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5/19 [======>.......................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 14:03:23.395478: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 6s 26ms/step\n",
      "13/13 [==============================] - 3s 40ms/step\n"
     ]
    }
   ],
   "source": [
    "DOMAIN_SPL = 600\n",
    "\n",
    "test_df = pd.read_json(TEST_P)\n",
    "test_prompt = pad_sequences(test_df[\"prompt\"], padding=\"post\", maxlen=MAX_PRO_LEN)\n",
    "test_txt = pad_sequences(test_df[\"txt\"], padding=\"post\", maxlen=MAX_TXT_LEN)\n",
    "\n",
    "print(test_prompt)\n",
    "\n",
    "model_1 = tf.keras.models.load_model(\"combined_model.h5\", custom_objects={ 'f1_loss': f1_loss })\n",
    "model_2 = tf.keras.models.load_model(\"combined_model_weight.h5\", custom_objects={ 'f1_loss': f1_loss })\n",
    "\n",
    "pred = []\n",
    "pred += model_1.predict([test_prompt[:DOMAIN_SPL], test_txt[:DOMAIN_SPL]]).tolist()\n",
    "pred += model_2.predict([test_prompt[DOMAIN_SPL:], test_txt[DOMAIN_SPL:]]).tolist()\n",
    "pred = [int(i) for i in np.round(pred).flatten()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(pred)\n",
    "pred_df.columns = [\"Predicted\"]\n",
    "pred_df.index.names = ['Id']\n",
    "\n",
    "pred_df.to_csv(\"./data/result2.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 12:59:15.978710: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-19 12:59:15.979581: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-19 12:59:15.980598: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-19 12:59:17.032635: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-19 12:59:17.033154: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-19 12:59:17.033190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-04-19 12:59:17.033689: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-19 12:59:17.033945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3383 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2023-04-19 12:59:25.395434: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-04-19 12:59:25.604229: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.5814876556396484\n",
      "f1-score:  -0.46047720313072205\n",
      "accuracy 0.8568571209907532\n",
      "110/110 [==============================] - 5s 21ms/step\n",
      "[[1553  197]\n",
      " [ 304 1446]]\n"
     ]
    }
   ],
   "source": [
    "combined_model_1 = tf.keras.models.load_model(\"combined_model.h5\", custom_objects={ 'f1_loss': f1_loss })\n",
    "## evaluate\n",
    "loss, accuracy, f1 = combined_model_1.evaluate([domain_1.test_prompt, domain_1.test_txt], domain_1.test_label, verbose=False)\n",
    "print(\"loss: \", loss)\n",
    "print(\"f1-score: \", f1)\n",
    "print(\"accuracy\", accuracy)\n",
    "comb_1_pre_rnn = combined_model_1.predict([domain_1.test_prompt, domain_1.test_txt])\n",
    "comb_1_pre_rnn = np.round(comb_1_pre_rnn).flatten()\n",
    "confusion = confusion_matrix(domain_1.test_label, comb_1_pre_rnn)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.31285056471824646\n",
      "accuracy 0.8899999856948853\n",
      "4/4 [==============================] - 3s 46ms/step\n",
      "[[49  1]\n",
      " [ 9 41]]\n",
      "f1-score:  0.8913043478260869\n"
     ]
    }
   ],
   "source": [
    "combined_model_2 = tf.keras.models.load_model(\"combined_model_weight.h5\", custom_objects={ 'f1_loss': f1_loss })\n",
    "## evaluate\n",
    "loss, accuracy, f1 = combined_model_2.evaluate([domain_2.test_prompt, domain_2.test_txt], domain_2.test_label, verbose=False)\n",
    "print(\"loss: \", loss)\n",
    "print(\"accuracy\", accuracy)\n",
    "comb_2_pre_rnn = combined_model_2.predict([domain_2.test_prompt, domain_2.test_txt])\n",
    "comb_2_pre_rnn = np.round(comb_2_pre_rnn).flatten()\n",
    "confusion = confusion_matrix(domain_2.test_label, comb_2_pre_rnn)\n",
    "print(confusion)\n",
    "f1 = f1_score(domain_2.test_label, comb_2_pre_rnn)\n",
    "print(\"f1-score: \", f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
