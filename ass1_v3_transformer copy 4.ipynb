{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 10:21:48.883480: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-24 10:21:49.071009: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-24 10:21:49.878033: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Using GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 10:21:51.227615: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 10:21:51.352715: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 10:21:51.353335: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import backend\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "from keras import layers\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Layer\n",
    "from tensorflow.keras.layers import Embedding, Input, GlobalAveragePooling1D, Dense, Concatenate\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "NUM_TOKEN = 5003\n",
    "MAX_PRO_LEN = 64\n",
    "MAX_TXT_LEN = 256\n",
    "NO_EPO = 60\n",
    "NO_BAT = 128\n",
    "CLS = 5001\n",
    "SEP = 5002\n",
    "PAD_ID = 0\n",
    "\n",
    "MACHINE_1_P = \"./data/set1_machine.json\"\n",
    "HUMAN_1_P = \"./data/set1_human.json\"\n",
    "MACHINE_2_P = \"./data/set2_machine.json\"\n",
    "HUMAN_2_P = \"./data/set2_human.json\"\n",
    "TEST_P = \"./data/test.json\"\n",
    "RANDOM_SEED = 42\n",
    "MACHINE_IND = 0\n",
    "HUMAN_IND = 1\n",
    "TEST_FRA = 0.01\n",
    "\n",
    "class DomainData:\n",
    "    \"\"\"\n",
    "    train_test_split, pad_sequence, PCA, class_weight\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def t_t_spli(self, test_size, random_state):\n",
    "        self.random_state = random_state\n",
    "        self.train_x, self.test_x, self.train_y, self.test_y = train_test_split(self.x, self.y, test_size=test_size, stratify = self.y, random_state = random_state)\n",
    "        self.train_x = self.train_x.reset_index(drop=True)\n",
    "        self.train_y = self.train_y.reset_index(drop=True)\n",
    "        self.test_x = self.test_x.reset_index(drop=True)\n",
    "        self.test_y = self.test_y.reset_index(drop=True)\n",
    "        \n",
    "    def add_sep(self):\n",
    "        self.x[\"prompt\"] = self.x.apply(lambda x: [i+1 for i in x[\"prompt\"]] , axis = 1)\n",
    "        self.x[\"prompt\"] = self.x.apply(lambda x: [CLS] + x[\"prompt\"] , axis = 1)\n",
    "        self.x[\"prompt\"] = self.x.apply(lambda x: x[\"prompt\"] + [SEP] if len(x[\"prompt\"])<MAX_PRO_LEN else x[\"prompt\"][:MAX_PRO_LEN-1] + [SEP], axis = 1)\n",
    "\n",
    "        self.x[\"txt\"] = self.x.apply(lambda x: [i+1 for i in x[\"txt\"]] , axis = 1)\n",
    "        self.x[\"txt\"] = self.x.apply(lambda x: [CLS] + x[\"txt\"] , axis = 1)\n",
    "        self.x[\"txt\"] = self.x.apply(lambda x: x[\"txt\"] + [SEP] if len(x[\"prompt\"])<MAX_TXT_LEN else x[\"txt\"][:MAX_TXT_LEN-1] + [SEP], axis = 1)\n",
    "        \n",
    "    \n",
    "    def add_padding(self, padding, prompt_len, txt_len):\n",
    "        self.train_prompt = self.train_x[\"prompt\"]\n",
    "        self.train_txt = self.train_x[\"txt\"]\n",
    "        self.train_label = self.train_y.to_numpy()\n",
    "        self.test_prompt = self.test_x[\"prompt\"]\n",
    "        self.test_txt = self.test_x[\"txt\"]\n",
    "        self.test_label = self.test_y.to_numpy()\n",
    "        unique_classes = np.unique(self.train_label)\n",
    "        class_weights = class_weight.compute_class_weight(\"balanced\", classes=unique_classes, y=self.train_y)\n",
    "        self.class_weights = dict(zip(unique_classes, class_weights))\n",
    "        \n",
    "        self.prompt_len = prompt_len\n",
    "        self.txt_len = txt_len\n",
    "        \n",
    "        self.train_prompt = pad_sequences(self.train_prompt, padding=padding, maxlen=prompt_len, value=PAD_ID)\n",
    "        self.train_txt = pad_sequences(self.train_txt, padding=padding, maxlen=txt_len, value=PAD_ID)\n",
    "        self.test_prompt = pad_sequences(self.test_prompt, padding=padding, maxlen=prompt_len, value=PAD_ID)\n",
    "        self.test_txt = pad_sequences(self.test_txt, padding=padding, maxlen=txt_len, value=PAD_ID)\n",
    "        \n",
    "        \n",
    "    def down_sampling(self):\n",
    "        mac_ind = self.train_y[self.train_y == MACHINE_IND].index.to_list()\n",
    "        hum_ind = self.train_y[self.train_y == HUMAN_IND].index.to_list()\n",
    "        lower = min(len(mac_ind), len(hum_ind))\n",
    "        sel_lit = mac_ind[:lower] + hum_ind[:lower]\n",
    "        self.train_x = self.train_x.iloc[sel_lit]\n",
    "        self.train_y = self.train_y.iloc[sel_lit]\n",
    "        random.shuffle(sel_lit)\n",
    "\n",
    "    def over_sampling(self, upper_fra):\n",
    "        mac_ind = self.train_y[self.train_y == MACHINE_IND].index.to_list()\n",
    "        hum_ind = self.train_y[self.train_y == HUMAN_IND].index.to_list()\n",
    "        lower = min(len(mac_ind), len(hum_ind))\n",
    "        if lower == len(mac_ind):\n",
    "            upper = int(lower*upper_fra) if lower*upper_fra < len(hum_ind) else len(hum_ind)\n",
    "            major = hum_ind[:upper]\n",
    "            minor = mac_ind[:lower]\n",
    "    \n",
    "        else:\n",
    "            upper = int(lower*upper_fra) if lower*upper_fra < len(mac_ind) else len(mac_ind)\n",
    "            major = mac_ind[:upper]\n",
    "            minor = hum_ind[:lower]\n",
    "        \n",
    "        add_n = upper - lower\n",
    "        oversampled = []\n",
    "        while(len(oversampled) < add_n):\n",
    "            oversampled.append(random.choice(mac_ind))\n",
    "        sel_lit = major + minor + oversampled\n",
    "        random.shuffle(sel_lit)\n",
    "        \n",
    "        self.train_x = self.train_x.iloc[sel_lit]\n",
    "        self.train_y = self.train_y.iloc[sel_lit]\n",
    "    \n",
    "    def test_down(self, frac = 1):\n",
    "        mac_ind = self.test_y[self.test_y == MACHINE_IND].index.to_list()\n",
    "        hum_ind = self.test_y[self.test_y == HUMAN_IND].index.to_list()\n",
    "        lower = min(len(mac_ind), len(hum_ind))\n",
    "        if frac > 1:\n",
    "            sel_lit = mac_ind[:lower] + hum_ind[:int(lower/frac)]\n",
    "        else:\n",
    "            sel_lit = mac_ind[:int(lower*frac)] + hum_ind[:lower]\n",
    "        self.test_x = self.test_x.iloc[sel_lit]\n",
    "        self.test_y = self.test_y.iloc[sel_lit]\n",
    "        random.shuffle(sel_lit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    # Calculate precision and recall\n",
    "    tp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)))\n",
    "    fp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)))\n",
    "    fn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)))\n",
    "    precision = tp / (tp + fp + backend.epsilon())\n",
    "    recall = tp / (tp + fn + backend.epsilon())\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1_score = 2 * precision * recall / (precision + recall + backend.epsilon())\n",
    "    \n",
    "    # Return negative F1 score as the loss (to minimize it)\n",
    "    return -f1_score\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Layer, Dense\n",
    "from tensorflow.keras.layers import Embedding, Input, GlobalAveragePooling1D, Dense\n",
    "import numpy as np\n",
    "\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = Sequential(\n",
    "            [Dense(ff_dim, activation=\"relu\"), \n",
    "             Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class TokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim, mask_zero=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience = 3)\n",
    "random.seed(RANDOM_SEED)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "if len(tf.config.list_physical_devices('GPU')):\n",
    "    tf.config.list_physical_devices('GPU')\n",
    "    print(\"Using GPU\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Improvement:\n",
    "- max_prompt/txt\n",
    "- batch\n",
    "- hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_226154/1117775470.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.x[\"prompt\"] = self.x.apply(lambda x: [i+1 for i in x[\"prompt\"]] , axis = 1)\n",
      "/tmp/ipykernel_226154/1117775470.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.x[\"prompt\"] = self.x.apply(lambda x: [CLS] + x[\"prompt\"] , axis = 1)\n",
      "/tmp/ipykernel_226154/1117775470.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.x[\"prompt\"] = self.x.apply(lambda x: x[\"prompt\"] + [SEP] if len(x[\"prompt\"])<MAX_PRO_LEN else x[\"prompt\"][:MAX_PRO_LEN-1] + [SEP], axis = 1)\n",
      "/tmp/ipykernel_226154/1117775470.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.x[\"txt\"] = self.x.apply(lambda x: [i+1 for i in x[\"txt\"]] , axis = 1)\n",
      "/tmp/ipykernel_226154/1117775470.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.x[\"txt\"] = self.x.apply(lambda x: [CLS] + x[\"txt\"] , axis = 1)\n",
      "/tmp/ipykernel_226154/1117775470.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.x[\"txt\"] = self.x.apply(lambda x: x[\"txt\"] + [SEP] if len(x[\"prompt\"])<MAX_TXT_LEN else x[\"txt\"][:MAX_TXT_LEN-1] + [SEP], axis = 1)\n"
     ]
    }
   ],
   "source": [
    "## _______________ Read data from domain 1 _______________\n",
    "man_1_df = pd.read_json(HUMAN_1_P)\n",
    "man_1_df[\"label\"] = HUMAN_IND\n",
    "mac_1_df = pd.read_json(MACHINE_1_P).drop(\"machine_id\", axis = 1)\n",
    "mac_1_df[\"label\"] = MACHINE_IND\n",
    "domain_1_df = pd.concat([man_1_df, mac_1_df])\n",
    "\n",
    "domain_1 = DomainData(domain_1_df[[\"prompt\", \"txt\"]], domain_1_df[\"label\"])\n",
    "domain_1.add_sep()\n",
    "domain_1.t_t_spli(TEST_FRA, RANDOM_SEED)\n",
    "domain_1.over_sampling(1.6)\n",
    "domain_1.test_down()\n",
    "domain_1.add_padding('post', MAX_PRO_LEN, MAX_TXT_LEN)\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "train_y = encoder.fit_transform(np.array(domain_1.train_label).reshape(-1, 1)).toarray()\n",
    "test_y = encoder.fit_transform(np.array(domain_1.test_label).reshape(-1, 1)).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 03:33:10.933392: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 03:33:10.934029: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 03:33:10.934301: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 03:33:11.922726: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 03:33:11.923128: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 03:33:11.923147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-04-24 03:33:11.923620: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 03:33:11.923671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3383 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junhual1/.local/lib/python3.8/site-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n",
      "2023-04-24 03:33:16.034886: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-04-24 03:33:16.104051: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7fdd4803b420 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-04-24 03:33:16.104109: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2023-04-24 03:33:16.140353: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-04-24 03:33:16.427557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-04-24 03:33:16.695757: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278/278 [==============================] - 61s 195ms/step - loss: 0.3172 - f1_loss: -0.8702 - val_loss: 0.2486 - val_f1_loss: -0.8968\n",
      "Epoch 2/60\n",
      "278/278 [==============================] - 24s 85ms/step - loss: 0.1926 - f1_loss: -0.9216 - val_loss: 0.2571 - val_f1_loss: -0.8973\n",
      "Epoch 3/60\n",
      "278/278 [==============================] - 18s 63ms/step - loss: 0.1143 - f1_loss: -0.9582 - val_loss: 0.1669 - val_f1_loss: -0.9429\n",
      "Epoch 4/60\n",
      "278/278 [==============================] - 16s 58ms/step - loss: 0.0613 - f1_loss: -0.9774 - val_loss: 0.1682 - val_f1_loss: -0.9467\n",
      "Epoch 5/60\n",
      "278/278 [==============================] - 13s 46ms/step - loss: 0.0418 - f1_loss: -0.9859 - val_loss: 0.1330 - val_f1_loss: -0.9519\n",
      "Epoch 6/60\n",
      "278/278 [==============================] - 11s 39ms/step - loss: 0.0258 - f1_loss: -0.9919 - val_loss: 0.2720 - val_f1_loss: -0.9421\n",
      "Epoch 7/60\n",
      "278/278 [==============================] - 11s 41ms/step - loss: 0.0153 - f1_loss: -0.9951 - val_loss: 0.1908 - val_f1_loss: -0.9577\n",
      "Epoch 8/60\n",
      "278/278 [==============================] - 11s 41ms/step - loss: 0.0096 - f1_loss: -0.9972 - val_loss: 0.2997 - val_f1_loss: -0.9458\n",
      "Epoch 9/60\n",
      "278/278 [==============================] - 11s 38ms/step - loss: 0.0054 - f1_loss: -0.9987 - val_loss: 0.2707 - val_f1_loss: -0.9561\n",
      "Epoch 10/60\n",
      "278/278 [==============================] - 11s 40ms/step - loss: 0.0016 - f1_loss: -0.9995 - val_loss: 0.3315 - val_f1_loss: -0.9492\n",
      "Epoch 11/60\n",
      "278/278 [==============================] - 11s 41ms/step - loss: 0.0125 - f1_loss: -0.9957 - val_loss: 0.2364 - val_f1_loss: -0.9558\n",
      "Epoch 12/60\n",
      "278/278 [==============================] - 10s 37ms/step - loss: 0.0122 - f1_loss: -0.9961 - val_loss: 0.2419 - val_f1_loss: -0.9558\n",
      "Epoch 13/60\n",
      "278/278 [==============================] - 11s 40ms/step - loss: 0.0090 - f1_loss: -0.9969 - val_loss: 0.2750 - val_f1_loss: -0.9511\n",
      "Model Saved: trans_model.h5\n",
      "Model Loaded: trans_model.h5\n",
      "loss:  0.20544865727424622\n",
      "3/3 [==============================] - 22s 98ms/step\n",
      "[[33  2]\n",
      " [ 3 32]]\n",
      "f1-score:  0.9275362318840579\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Layer\n",
    "from tensorflow.keras.layers import Embedding, Input, GlobalAveragePooling1D, Dense, Concatenate\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
    "\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = Sequential(\n",
    "            [Dense(ff_dim, activation=\"relu\"), \n",
    "             Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class TokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim, mask_zero=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "vocab_size = 5000  # Only consider the top 20k words\n",
    "embed_dim = 128  # Embedding size for each token\n",
    "num_heads = 6  # Number of attention heads\n",
    "ff_dim = 64  # Hidden layer size in feed forward network inside transformer\n",
    "epo_size = NO_EPO\n",
    "batch_size = 32\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "# define model\n",
    "inputs_p = Input(shape=(MAX_PRO_LEN,))\n",
    "embedding_layer = TokenAndPositionEmbedding(MAX_PRO_LEN, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs_p)\n",
    "x = transformer_block(x)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(20, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "\n",
    "# define model\n",
    "inputs_t = Input(shape=(MAX_TXT_LEN,))\n",
    "embedding_layer = TokenAndPositionEmbedding(MAX_TXT_LEN, vocab_size, embed_dim)\n",
    "y = embedding_layer(inputs_t)\n",
    "y = transformer_block(y)\n",
    "y = GlobalAveragePooling1D()(y)\n",
    "y = Dropout(0.1)(y)\n",
    "y = Dense(20, activation=\"relu\")(y)\n",
    "y = Dropout(0.1)(y)\n",
    "\n",
    "\n",
    "# Concatenate outputs from prompt and text models\n",
    "merged = Concatenate()([x, y])\n",
    "merged = Dense(units=64, activation='relu')(merged)\n",
    "merged = Dense(20, activation=\"relu\")(merged)\n",
    "outputs = Dense(units=1, activation='sigmoid')(merged)\n",
    "trans_model_2 = Model(inputs=[inputs_p, inputs_t], outputs=outputs)\n",
    "\n",
    "# Compile and train\n",
    "trans_model_2.compile(optimizer=optimizer, loss=loss, metrics=[f1_loss])\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('trans_model.h5', monitor='val_loss', save_best_only=True)\n",
    "trans_model_2.fit([domain_1.train_prompt, domain_1.train_txt], domain_1.train_label, epochs=epo_size, batch_size=batch_size, validation_split=0.2, callbacks = [callback, model_checkpoint])\n",
    "# trans_model_2.fit([domain_1.train_prompt, domain_1.train_txt], domain_1.train_label, epochs=epo_size, batch_size=batch_size, class_weight = domain_1.class_weights, validation_split=0.2, callbacks = [callback, model_checkpoint])\n",
    "print(\"Model Saved: trans_model.h5\")\n",
    "\n",
    "trans_model_2 = tf.keras.models.load_model(\"trans_model.h5\", custom_objects={ 'f1_loss': f1_loss, 'TransformerBlock': TransformerBlock, 'TokenAndPositionEmbedding':TokenAndPositionEmbedding })\n",
    "print(\"Model Loaded: trans_model.h5\")\n",
    "loss, f1 = trans_model_2.evaluate([domain_1.test_prompt, domain_1.test_txt], domain_1.test_label, verbose=False)\n",
    "print(\"loss: \", loss)\n",
    "trans_1_pre_rnn = trans_model_2.predict([domain_1.test_prompt, domain_1.test_txt])\n",
    "trans_1_pre_rnn = np.round(trans_1_pre_rnn).flatten()\n",
    "confusion = confusion_matrix(domain_1.test_label, trans_1_pre_rnn)\n",
    "# trans_1_pre_rnn = [0 if i.flatten()[0] > i.flatten()[1] else 1 for i in trans_1_pre_rnn]\n",
    "# confusion = confusion_matrix(domain_1.test_label, trans_1_pre_rnn)\n",
    "print(confusion)\n",
    "f1 = f1_score(domain_1.test_label, trans_1_pre_rnn)\n",
    "print(\"f1-score: \", f1)\n",
    "\n",
    "# 31/33 -> Adam\n",
    "# 34/29 -> SGD\n",
    "# 33/31 -> RMSprop\n",
    "# 34/29 -> Adagrad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trans_model_2 = tf.keras.models.load_model(\"trans_model.h5\", custom_objects={ 'f1_loss': f1_loss, 'TransformerBlock': TransformerBlock, 'TokenAndPositionEmbedding':TokenAndPositionEmbedding })\n",
    "# print(\"Model Loaded: trans_model.h5\")\n",
    "# loss, f1 = trans_model_2.evaluate([domain_1.test_prompt, domain_1.test_txt], domain_1.test_label, verbose=False)\n",
    "# print(\"loss: \", loss)\n",
    "# trans_1_pre_rnn = trans_model_2.predict([domain_1.test_prompt, domain_1.test_txt])\n",
    "# trans_1_pre_rnn = np.round(trans_1_pre_rnn).flatten()\n",
    "# confusion = confusion_matrix(domain_1.test_label, trans_1_pre_rnn)\n",
    "# # trans_1_pre_rnn = [0 if i.flatten()[0] > i.flatten()[1] else 1 for i in trans_1_pre_rnn]\n",
    "# # confusion = confusion_matrix(domain_1.test_label, trans_1_pre_rnn)\n",
    "# print(confusion)\n",
    "# f1 = f1_score(domain_1.test_label, trans_1_pre_rnn)\n",
    "# print(\"f1-score: \", f1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain 2 weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_236438/1117775470.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.x[\"prompt\"] = self.x.apply(lambda x: [i+1 for i in x[\"prompt\"]] , axis = 1)\n",
      "/tmp/ipykernel_236438/1117775470.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.x[\"prompt\"] = self.x.apply(lambda x: [CLS] + x[\"prompt\"] , axis = 1)\n",
      "/tmp/ipykernel_236438/1117775470.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.x[\"prompt\"] = self.x.apply(lambda x: x[\"prompt\"] + [SEP] if len(x[\"prompt\"])<MAX_PRO_LEN else x[\"prompt\"][:MAX_PRO_LEN-1] + [SEP], axis = 1)\n",
      "/tmp/ipykernel_236438/1117775470.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.x[\"txt\"] = self.x.apply(lambda x: [i+1 for i in x[\"txt\"]] , axis = 1)\n",
      "/tmp/ipykernel_236438/1117775470.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.x[\"txt\"] = self.x.apply(lambda x: [CLS] + x[\"txt\"] , axis = 1)\n",
      "/tmp/ipykernel_236438/1117775470.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.x[\"txt\"] = self.x.apply(lambda x: x[\"txt\"] + [SEP] if len(x[\"prompt\"])<MAX_TXT_LEN else x[\"txt\"][:MAX_TXT_LEN-1] + [SEP], axis = 1)\n",
      "/tmp/ipykernel_236438/1117775470.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.x[\"prompt\"] = self.x.apply(lambda x: [i+1 for i in x[\"prompt\"]] , axis = 1)\n",
      "/tmp/ipykernel_236438/1117775470.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.x[\"prompt\"] = self.x.apply(lambda x: [CLS] + x[\"prompt\"] , axis = 1)\n",
      "/tmp/ipykernel_236438/1117775470.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.x[\"prompt\"] = self.x.apply(lambda x: x[\"prompt\"] + [SEP] if len(x[\"prompt\"])<MAX_PRO_LEN else x[\"prompt\"][:MAX_PRO_LEN-1] + [SEP], axis = 1)\n",
      "/tmp/ipykernel_236438/1117775470.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.x[\"txt\"] = self.x.apply(lambda x: [i+1 for i in x[\"txt\"]] , axis = 1)\n",
      "/tmp/ipykernel_236438/1117775470.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.x[\"txt\"] = self.x.apply(lambda x: [CLS] + x[\"txt\"] , axis = 1)\n",
      "/tmp/ipykernel_236438/1117775470.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.x[\"txt\"] = self.x.apply(lambda x: x[\"txt\"] + [SEP] if len(x[\"prompt\"])<MAX_TXT_LEN else x[\"txt\"][:MAX_TXT_LEN-1] + [SEP], axis = 1)\n"
     ]
    }
   ],
   "source": [
    "over_fra = 1.6\n",
    "weight_fra = 300\n",
    "\n",
    "## _______________ Read data from domain 1 _______________\n",
    "man_1_df = pd.read_json(HUMAN_1_P)\n",
    "man_1_df[\"label\"] = HUMAN_IND\n",
    "mac_1_df = pd.read_json(MACHINE_1_P).drop(\"machine_id\", axis = 1)\n",
    "mac_1_df[\"label\"] = MACHINE_IND\n",
    "domain_1_df = pd.concat([man_1_df, mac_1_df])\n",
    "\n",
    "domain_1 = DomainData(domain_1_df[[\"prompt\", \"txt\"]], domain_1_df[\"label\"])\n",
    "domain_1.add_sep()\n",
    "domain_1.t_t_spli(TEST_FRA, RANDOM_SEED)\n",
    "# domain_1.down_sampling()\n",
    "domain_1.add_padding('post', MAX_PRO_LEN, MAX_TXT_LEN)\n",
    "\n",
    "## _______________ Read data from domain 2 _______________\n",
    "man_2_df = pd.read_json(HUMAN_2_P)\n",
    "man_2_df[\"label\"] = HUMAN_IND\n",
    "mac_2_df = pd.read_json(MACHINE_2_P).drop(\"machine_id\", axis = 1)\n",
    "mac_2_df[\"label\"] = MACHINE_IND\n",
    "domain_2_df = pd.concat([man_2_df, mac_2_df])\n",
    "\n",
    "domain_2 = DomainData(domain_2_df[[\"prompt\", \"txt\"]], domain_2_df[\"label\"])\n",
    "domain_2.add_sep()\n",
    "domain_2.t_t_spli(TEST_FRA, RANDOM_SEED)\n",
    "# domain_2.over_sampling(over_fra)\n",
    "domain_2.test_down()\n",
    "domain_2.add_padding('post', MAX_PRO_LEN, MAX_TXT_LEN)\n",
    "\n",
    "\n",
    "\n",
    "## _______________ weight data _______________\n",
    "sample_weight_1 = np.ones(len(domain_1.train_label))\n",
    "sample_weight_2 = np.ones(len(domain_2.train_label))\n",
    "sample_weight_2 *= weight_fra\n",
    "sample_weight = np.concatenate([sample_weight_1, sample_weight_2])\n",
    "\n",
    "train_prompt = np.concatenate([domain_1.train_prompt, domain_2.train_prompt])\n",
    "train_txt = np.concatenate([domain_1.train_txt, domain_2.train_txt])\n",
    "train_label = np.concatenate([domain_1.train_label, domain_2.train_label])\n",
    "\n",
    "data = list(zip(train_prompt, train_txt, train_label, sample_weight))\n",
    "random.shuffle(data)\n",
    "\n",
    "train_prompt, train_txt, train_label, sample_weight = zip(*data)\n",
    "train_prompt = np.array(train_prompt)\n",
    "train_txt = np.array(train_txt)\n",
    "train_label = np.array(train_label)\n",
    "sample_weight = np.array(sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 03:49:43.504718: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 03:49:43.505385: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 03:49:43.506106: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 03:49:44.662775: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 03:49:44.663839: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 03:49:44.663859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-04-24 03:49:44.664664: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 03:49:44.664715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3383 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 03:50:03.608722: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-04-24 03:50:03.609005: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-04-24 03:50:03.934291: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f592790af60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-04-24 03:50:03.934343: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2023-04-24 03:50:03.994070: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-04-24 03:50:04.309967: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-04-24 03:50:04.631278: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784/784 [==============================] - ETA: 0s - loss: 1.1511 - f1_loss: -0.9493WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 139s 168ms/step - loss: 1.1511 - f1_loss: -0.9493 - val_loss: 0.8675 - val_f1_loss: -0.9854\n",
      "Epoch 2/60\n",
      "784/784 [==============================] - ETA: 0s - loss: 0.6174 - f1_loss: -0.9807WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 96s 122ms/step - loss: 0.6174 - f1_loss: -0.9807 - val_loss: 0.4454 - val_f1_loss: -0.9843\n",
      "Epoch 3/60\n",
      "784/784 [==============================] - ETA: 0s - loss: 0.4028 - f1_loss: -0.9826WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 94s 119ms/step - loss: 0.4028 - f1_loss: -0.9826 - val_loss: 0.3136 - val_f1_loss: -0.9855\n",
      "Epoch 4/60\n",
      "784/784 [==============================] - ETA: 0s - loss: 0.2459 - f1_loss: -0.9860WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 92s 118ms/step - loss: 0.2459 - f1_loss: -0.9860 - val_loss: 0.3402 - val_f1_loss: -0.9875\n",
      "Epoch 5/60\n",
      "784/784 [==============================] - ETA: 0s - loss: 0.1106 - f1_loss: -0.9878WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 93s 118ms/step - loss: 0.1106 - f1_loss: -0.9878 - val_loss: 0.1517 - val_f1_loss: -0.9897\n",
      "Epoch 6/60\n",
      "784/784 [==============================] - ETA: 0s - loss: 0.0768 - f1_loss: -0.9895WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 93s 119ms/step - loss: 0.0768 - f1_loss: -0.9895 - val_loss: 0.1715 - val_f1_loss: -0.9907\n",
      "Epoch 7/60\n",
      "784/784 [==============================] - ETA: 0s - loss: 0.0640 - f1_loss: -0.9910WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 92s 118ms/step - loss: 0.0640 - f1_loss: -0.9910 - val_loss: 0.1559 - val_f1_loss: -0.9912\n",
      "Epoch 8/60\n",
      "784/784 [==============================] - ETA: 0s - loss: 0.0619 - f1_loss: -0.9914WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 91s 117ms/step - loss: 0.0619 - f1_loss: -0.9914 - val_loss: 0.2429 - val_f1_loss: -0.9909\n",
      "Epoch 9/60\n",
      "784/784 [==============================] - ETA: 0s - loss: 0.1030 - f1_loss: -0.9901WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 92s 117ms/step - loss: 0.1030 - f1_loss: -0.9901 - val_loss: 0.2169 - val_f1_loss: -0.9913\n",
      "Epoch 10/60\n",
      "784/784 [==============================] - ETA: 0s - loss: 0.0496 - f1_loss: -0.9925WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 92s 118ms/step - loss: 0.0496 - f1_loss: -0.9925 - val_loss: 0.2411 - val_f1_loss: -0.9924\n",
      "Epoch 11/60\n",
      "784/784 [==============================] - ETA: 0s - loss: 0.0354 - f1_loss: -0.9941WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 92s 118ms/step - loss: 0.0354 - f1_loss: -0.9941 - val_loss: 0.3285 - val_f1_loss: -0.9925\n",
      "Epoch 12/60\n",
      "784/784 [==============================] - ETA: 0s - loss: 0.0295 - f1_loss: -0.9950WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 91s 116ms/step - loss: 0.0295 - f1_loss: -0.9950 - val_loss: 0.3914 - val_f1_loss: -0.9931\n",
      "Epoch 13/60\n",
      "784/784 [==============================] - ETA: 0s - loss: 0.0247 - f1_loss: -0.9954WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 92s 118ms/step - loss: 0.0247 - f1_loss: -0.9954 - val_loss: 0.3556 - val_f1_loss: -0.9929\n",
      "Epoch 14/60\n",
      "784/784 [==============================] - ETA: 0s - loss: 0.0887 - f1_loss: -0.9947WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 91s 116ms/step - loss: 0.0887 - f1_loss: -0.9947 - val_loss: 0.8600 - val_f1_loss: -0.9871\n",
      "Epoch 15/60\n",
      "784/784 [==============================] - ETA: 0s - loss: 0.0742 - f1_loss: -0.9929WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 91s 116ms/step - loss: 0.0742 - f1_loss: -0.9929 - val_loss: 0.7884 - val_f1_loss: -0.9919\n",
      "Epoch 16/60\n",
      "784/784 [==============================] - ETA: 0s - loss: 0.0307 - f1_loss: -0.9950WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "784/784 [==============================] - 92s 117ms/step - loss: 0.0307 - f1_loss: -0.9950 - val_loss: 0.5530 - val_f1_loss: -0.9937\n",
      "Model Saved: trans_model_weighted.h5\n",
      "Model Loaded: trans_model_weighted.h5\n",
      "loss:  0.037508413195610046\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "[[1 0]\n",
      " [0 1]]\n",
      "f1-score:  1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Layer\n",
    "from tensorflow.keras.layers import Embedding, Input, GlobalAveragePooling1D, Dense, Concatenate\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
    "\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = Sequential(\n",
    "            [Dense(ff_dim, activation=\"relu\"), \n",
    "             Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class TokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim, mask_zero=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "vocab_size = 5000  # Only consider the top 20k words\n",
    "embed_dim = 128  # Embedding size for each token\n",
    "num_heads = 8  # Number of attention heads\n",
    "ff_dim = 20  # Hidden layer size in feed forward network inside transformer\n",
    "epo_size = NO_EPO\n",
    "batch_size = 128\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "\n",
    "# define model\n",
    "inputs_p = Input(shape=(MAX_PRO_LEN,))\n",
    "embedding_layer = TokenAndPositionEmbedding(MAX_PRO_LEN, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs_p)\n",
    "x = transformer_block(x)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(16, activation=\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "\n",
    "# define model\n",
    "inputs_t = Input(shape=(MAX_TXT_LEN,))\n",
    "embedding_layer = TokenAndPositionEmbedding(MAX_TXT_LEN, vocab_size, embed_dim)\n",
    "y = embedding_layer(inputs_t)\n",
    "y = transformer_block(y)\n",
    "y = GlobalAveragePooling1D()(y)\n",
    "y = Dropout(0.5)(y)\n",
    "y = Dense(16, activation=\"relu\")(y)\n",
    "y = Dropout(0.5)(y)\n",
    "\n",
    "\n",
    "# Concatenate outputs from prompt and text models\n",
    "merged = Concatenate()([x, y])\n",
    "merged = Dense(units=64, activation='relu')(merged)\n",
    "merged = Dense(20, activation=\"relu\")(merged)\n",
    "outputs = Dense(units=1, activation='sigmoid')(merged)\n",
    "trans_model_2 = Model(inputs=[inputs_p, inputs_t], outputs=outputs)\n",
    "\n",
    "# Compile and train\n",
    "trans_model_2.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[f1_loss])\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('trans_model_weighted.h5', monitor='val_loss', save_best_only=True)\n",
    "trans_model_2.fit([train_prompt, train_txt], train_label, epochs=epo_size, batch_size=batch_size, sample_weight = sample_weight, validation_split=0.2, callbacks = [callback, model_checkpoint])\n",
    "print(\"Model Saved: trans_model_weighted.h5\")\n",
    "\n",
    "trans_model_2 = tf.keras.models.load_model(\"trans_model_weighted.h5\", custom_objects={ 'f1_loss': f1_loss, 'TransformerBlock': TransformerBlock, 'TokenAndPositionEmbedding':TokenAndPositionEmbedding })\n",
    "print(\"Model Loaded: trans_model_weighted.h5\")\n",
    "loss, f1 = trans_model_2.evaluate([domain_2.test_prompt, domain_2.test_txt], domain_2.test_label, verbose=False)\n",
    "print(\"loss: \", loss)\n",
    "trans_2_pre_rnn = trans_model_2.predict([domain_2.test_prompt, domain_2.test_txt])\n",
    "trans_2_pre_rnn = np.round(trans_2_pre_rnn).flatten()\n",
    "confusion = confusion_matrix(domain_2.test_label, trans_2_pre_rnn)\n",
    "print(confusion)\n",
    "f1 = f1_score(domain_2.test_label, trans_2_pre_rnn)\n",
    "print(\"f1-score: \", f1)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 1s 18ms/step\n",
      "13/13 [==============================] - 0s 14ms/step\n"
     ]
    }
   ],
   "source": [
    "DOMAIN_SPL = 600\n",
    "\n",
    "test_df = pd.read_json(TEST_P)\n",
    "test_prompt = pad_sequences(test_df[\"prompt\"], padding=\"post\", maxlen=MAX_PRO_LEN)\n",
    "test_txt = pad_sequences(test_df[\"txt\"], padding=\"post\", maxlen=MAX_TXT_LEN)\n",
    "\n",
    "model_1 = tf.keras.models.load_model(\"trans_model.h5\", custom_objects={ 'f1_loss': f1_loss, 'TransformerBlock': TransformerBlock, 'TokenAndPositionEmbedding':TokenAndPositionEmbedding })\n",
    "model_2 = tf.keras.models.load_model(\"trans_model_weighted.h5\", custom_objects={ 'f1_loss': f1_loss, 'TransformerBlock': TransformerBlock, 'TokenAndPositionEmbedding':TokenAndPositionEmbedding })\n",
    "\n",
    "pred = []\n",
    "pred += model_1.predict([test_prompt[:DOMAIN_SPL], test_txt[:DOMAIN_SPL]]).tolist()\n",
    "pred += model_2.predict([test_prompt[DOMAIN_SPL:], test_txt[DOMAIN_SPL:]]).tolist()\n",
    "pred = [int(i) for i in np.round(pred).flatten()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(pred)\n",
    "pred_df.columns = [\"Predicted\"]\n",
    "pred_df.index.names = ['Id']\n",
    "\n",
    "pred_df.to_csv(\"./data/result3_v7.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ori: transformer--tunned\n",
    "# v1: config tunning\n",
    "# v2: add sep, test_fra: 0.2 -- 0.582\n",
    "# v3: test_fra = 0.5, retrain domain 2\n",
    "# v4: test_fra = 0.5, retrain domain 1\n",
    "# v5: test_fra = 0.5, class weight\n",
    "# v6: test_fra = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference:  72\n",
      "895\n",
      "935\n",
      "[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0\n",
      " 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0\n",
      " 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0]\n",
      "[1, 22, 24, 32, 38, 39, 43, 52, 64, 67, 69, 72, 74, 81, 87, 92, 99, 107, 126, 128, 130, 134, 136, 141, 154, 155, 167, 174, 179, 182, 187, 211, 226, 240, 253, 264, 300, 310, 335, 353, 372, 393, 396, 398, 435, 437, 447, 450, 461, 474, 477, 494, 495, 508, 524, 530, 545, 552, 555, 556, 557, 582, 583, 588, 592, 597, 598, 606, 651, 805, 942, 997]\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"./data/result3_v2.csv\")\n",
    "df2 = pd.read_csv(\"./data/result3_v2.csv\")\n",
    "\n",
    "df1[\"pred_2\"] = df2[\"Predicted\"]\n",
    "df1[\"compare\"] = df1.apply(lambda x: 1 if int(x[\"pred_2\"] != x[\"Predicted\"]) else 0, axis=1)\n",
    "\n",
    "print(\"Difference: \", sum(df1[\"compare\"].values.tolist()))\n",
    "\n",
    "print(sum(df1[\"Predicted\"].to_numpy()))\n",
    "print(sum(df1[\"pred_2\"].to_numpy()))\n",
    "\n",
    "print(df1[\"compare\"].values)\n",
    "dis_m = []\n",
    "for i in range(len(df1[\"compare\"].values)):\n",
    "    if df1[\"compare\"][i] != 0:\n",
    "        dis_m.append(i)\n",
    "print(dis_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference:  11\n",
      "895\n",
      "884\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0]\n",
      "[606, 651, 656, 683, 787, 804, 805, 816, 883, 942, 967]\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"./data/result3_v2.csv\")\n",
    "df2 = pd.read_csv(\"./data/result3_v3.csv\")\n",
    "\n",
    "df1[\"pred_2\"] = df2[\"Predicted\"]\n",
    "df1[\"compare\"] = df1.apply(lambda x: 1 if int(x[\"pred_2\"] != x[\"Predicted\"]) else 0, axis=1)\n",
    "\n",
    "print(\"Difference: \", sum(df1[\"compare\"].values.tolist()))\n",
    "\n",
    "print(sum(df1[\"Predicted\"].to_numpy()))\n",
    "print(sum(df1[\"pred_2\"].to_numpy()))\n",
    "\n",
    "print(df1[\"compare\"].values)\n",
    "dis_m = []\n",
    "for i in range(len(df1[\"compare\"].values)):\n",
    "    if df1[\"compare\"][i] != 0:\n",
    "        dis_m.append(i)\n",
    "print(dis_m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
