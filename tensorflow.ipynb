{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0f8a26c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils import class_weight, resample\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, Embedding, Dense, Dropout, LSTM, GRU\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8b371608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set1 - human/machine : 122584 / 3500\n",
      "set2 - human/machine : 100 / 400\n",
      "\n",
      "Mock test sets\n",
      "extract set1 - human/machine : 122284 / 3200\n",
      "downsample set1 - human/machine : 3200 / 3200\n",
      "generate 1000 mock tests.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "def json_to_dataset(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "set1_human = json_to_dataset(\"./data/set1_human.json\")\n",
    "set1_machine = json_to_dataset(\"./data/set1_machine.json\")\n",
    "set1_machine = set1_machine.drop('machine_id', axis=1)\n",
    "\n",
    "set2_human = json_to_dataset(\"./data/set2_human.json\")\n",
    "set2_machine = json_to_dataset(\"./data/set2_machine.json\")\n",
    "set2_machine = set2_machine.drop('machine_id', axis=1)\n",
    "\n",
    "print(f\"set1 - human/machine : {len(set1_human)} / {len(set1_machine)}\")\n",
    "print(f\"set2 - human/machine : {len(set2_human)} / {len(set2_machine)}\")\n",
    "\n",
    "# Label data and combine\n",
    "set1_human[\"label\"] = 1\n",
    "set1_machine[\"label\"] = 0\n",
    "\n",
    "print(\"\\nMock test sets\")\n",
    "# Domain 1 - 600 records\n",
    "sample_human = set1_human.sample(300, random_state=42)\n",
    "set1_human = set1_human.drop(sample_human.index)\n",
    "sample_machine = set1_machine.sample(300, random_state=42)\n",
    "set1_machine = set1_machine.drop(sample_machine.index)\n",
    "print(f\"extract set1 - human/machine : {len(set1_human)} / {len(set1_machine)}\")\n",
    "# Combine the samples into a mock test dataset\n",
    "mock_test_data1 = pd.concat([sample_human, sample_machine], ignore_index=True)\n",
    "X_mock_test = pad_sequences(mock_test_data1['txt'], maxlen=max_len)\n",
    "y_mock_test = mock_test_data1[\"label\"].values\n",
    "set1_human = set1_human.sample(len(set1_machine), random_state=42)\n",
    "dataset1 = pd.concat([set1_human, set1_machine], ignore_index=True)\n",
    "print(f\"downsample set1 - human/machine : {len(set1_human)} / {len(set1_machine)}\")\n",
    "\n",
    "set2_human[\"label\"] = 1\n",
    "set2_machine[\"label\"] = 0\n",
    "dataset2 = pd.concat([set2_human, set2_machine], ignore_index=True)\n",
    "# Domain 2 - 400 records\n",
    "# Sample 100 records from the human class\n",
    "sample_human = dataset2[dataset2[\"label\"] == 1]\n",
    "# Oversample the human records to reach 200 records\n",
    "sample_human_oversampled = resample(sample_human, replace=True, n_samples=200, random_state=42)\n",
    "# Sample 200 records from the machine class\n",
    "sample_machine = dataset2[dataset2[\"label\"] == 0].sample(200, random_state=42)\n",
    "# Combine the samples into a mock test dataset\n",
    "mock_test_data2 = pd.concat([sample_human_oversampled, sample_machine], ignore_index=True)\n",
    "# Preprocess the mock test dataset\n",
    "# X_mock_test2 = pad_sequences(mock_test_data2['txt'], maxlen=max_len)\n",
    "# y_mock_test2 = mock_test_data2[\"label\"].values\n",
    "\n",
    "# Combine the two mock test sets\n",
    "combined_mock_test = pd.concat([mock_test_data1, mock_test_data2], ignore_index=True)\n",
    "X_combined_mock_test = pad_sequences(combined_mock_test['txt'], maxlen=max_len)\n",
    "y_combined_mock_test = combined_mock_test[\"label\"].values\n",
    "print(f\"generate {len(y_combined_mock_test)} mock tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7ffb470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation split\n",
    "train_data1, val_data1 = train_test_split(dataset1, test_size=0.2, random_state=42, stratify=dataset1[\"label\"])\n",
    "train_data2, val_data2 = train_test_split(dataset2, test_size=0.2, random_state=42, stratify=dataset2[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bcb1d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed param\n",
    "vocab_size = 5000\n",
    "\n",
    "# model param\n",
    "max_len = 250\n",
    "embedding_dim = 64\n",
    "lstm_unit = 64\n",
    "\n",
    "# fit param\n",
    "num_epoch = 50\n",
    "num_batch = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8cf1c56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparation done.\n"
     ]
    }
   ],
   "source": [
    "strategy = \"txt\" # \"prompt\"\n",
    "X_train1 = pad_sequences(train_data1[strategy], maxlen=max_len)\n",
    "y_train1 = train_data1[\"label\"].values\n",
    "X_val1 = pad_sequences(val_data1[strategy], maxlen=max_len)\n",
    "y_val1 = val_data1[\"label\"].values\n",
    "\n",
    "X_train2 = pad_sequences(train_data2[strategy], maxlen=max_len)\n",
    "y_train2 = train_data2[\"label\"].values\n",
    "X_val2 = pad_sequences(val_data2[strategy], maxlen=max_len)\n",
    "y_val2 = val_data2[\"label\"].values\n",
    "\n",
    "# Set up the EarlyStopping and ModelCheckpoint callback\n",
    "checkpoint_filepath1 = './best_model1.h5'\n",
    "checkpoint_filepath2 = './best_model2.h5'\n",
    "model_checkpoint_callback1 = ModelCheckpoint(filepath=checkpoint_filepath1, save_best_only=True, \n",
    "                                             monitor='val_loss', mode='min')\n",
    "model_checkpoint_callback2 = ModelCheckpoint(filepath=checkpoint_filepath2, save_best_only=True, \n",
    "                                             monitor='val_loss', mode='min')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode='min')\n",
    "print(\"Preparation done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "161ad1bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain 1 model:\n",
      "Epoch 1/50\n",
      "80/80 [==============================] - 11s 70ms/step - loss: 0.4280 - accuracy: 0.8471 - val_loss: 0.3219 - val_accuracy: 0.8773\n",
      "Epoch 2/50\n",
      "80/80 [==============================] - 5s 58ms/step - loss: 0.3005 - accuracy: 0.8820 - val_loss: 0.3028 - val_accuracy: 0.8914\n",
      "Epoch 3/50\n",
      "80/80 [==============================] - 5s 58ms/step - loss: 0.2140 - accuracy: 0.9162 - val_loss: 0.3751 - val_accuracy: 0.8781\n",
      "Epoch 4/50\n",
      "80/80 [==============================] - 5s 57ms/step - loss: 0.1529 - accuracy: 0.9402 - val_loss: 0.3587 - val_accuracy: 0.8758\n",
      "Epoch 5/50\n",
      "80/80 [==============================] - 5s 57ms/step - loss: 0.1203 - accuracy: 0.9566 - val_loss: 0.3916 - val_accuracy: 0.8758\n",
      "Epoch 6/50\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 0.0814 - accuracy: 0.9727 - val_loss: 0.4861 - val_accuracy: 0.8250\n",
      "Epoch 7/50\n",
      "80/80 [==============================] - 4s 52ms/step - loss: 0.0712 - accuracy: 0.9770 - val_loss: 0.5257 - val_accuracy: 0.8313\n",
      "Epoch 8/50\n",
      "80/80 [==============================] - 4s 52ms/step - loss: 0.0577 - accuracy: 0.9818 - val_loss: 0.5079 - val_accuracy: 0.8625\n",
      "Epoch 9/50\n",
      "80/80 [==============================] - 4s 51ms/step - loss: 0.0336 - accuracy: 0.9908 - val_loss: 0.5595 - val_accuracy: 0.8523\n",
      "Epoch 10/50\n",
      "80/80 [==============================] - 4s 52ms/step - loss: 0.0187 - accuracy: 0.9967 - val_loss: 0.6297 - val_accuracy: 0.8273\n",
      "Epoch 11/50\n",
      "80/80 [==============================] - 4s 56ms/step - loss: 0.0134 - accuracy: 0.9969 - val_loss: 0.6287 - val_accuracy: 0.8258\n",
      "Epoch 12/50\n",
      "80/80 [==============================] - 4s 56ms/step - loss: 0.0107 - accuracy: 0.9988 - val_loss: 0.6915 - val_accuracy: 0.8172\n",
      "----------------------------------------\n",
      "Domain 2 model:\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 8s 395ms/step - loss: 0.6383 - accuracy: 0.6760 - val_loss: 0.5561 - val_accuracy: 0.8000\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 1s 79ms/step - loss: 0.5542 - accuracy: 0.7880 - val_loss: 0.5018 - val_accuracy: 0.8000\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.4899 - accuracy: 0.8000 - val_loss: 0.4468 - val_accuracy: 0.8000\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 64ms/step - loss: 0.4530 - accuracy: 0.8060 - val_loss: 0.3871 - val_accuracy: 0.8100\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.3951 - accuracy: 0.8200 - val_loss: 0.3021 - val_accuracy: 0.8500\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.3007 - accuracy: 0.8720 - val_loss: 0.2073 - val_accuracy: 0.9300\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 0.2021 - accuracy: 0.9340 - val_loss: 0.1450 - val_accuracy: 0.9500\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.1520 - accuracy: 0.9400 - val_loss: 0.1100 - val_accuracy: 0.9700\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.1028 - accuracy: 0.9680 - val_loss: 0.0568 - val_accuracy: 0.9800\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 64ms/step - loss: 0.0704 - accuracy: 0.9840 - val_loss: 0.0224 - val_accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 0.0536 - accuracy: 0.9900 - val_loss: 0.0143 - val_accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 0.0284 - accuracy: 0.9940 - val_loss: 0.0113 - val_accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 1s 74ms/step - loss: 0.0284 - accuracy: 0.9980 - val_loss: 0.0070 - val_accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.0251 - accuracy: 0.9980 - val_loss: 0.0064 - val_accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.0182 - accuracy: 0.9960 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.0116 - accuracy: 0.9980 - val_loss: 0.0038 - val_accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.0083 - accuracy: 0.9980 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.0058 - accuracy: 0.9980 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 9.3146e-04 - val_accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 8.7096e-04 - val_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 63ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 8.1393e-04 - val_accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 7.6096e-04 - val_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 63ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 7.1492e-04 - val_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 6.6754e-04 - val_accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 6.2236e-04 - val_accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 5.8820e-04 - val_accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 5.5860e-04 - val_accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 5.2813e-04 - val_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 4.9629e-04 - val_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.6898e-04 - val_accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.4612e-04 - val_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.2629e-04 - val_accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.0757e-04 - val_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 3.9034e-04 - val_accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 3.7193e-04 - val_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 3.5559e-04 - val_accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 1s 74ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.4040e-04 - val_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 9.5402e-04 - accuracy: 1.0000 - val_loss: 3.2837e-04 - val_accuracy: 1.0000\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 66ms/step - loss: 9.3566e-04 - accuracy: 1.0000 - val_loss: 3.1497e-04 - val_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 8.7450e-04 - accuracy: 1.0000 - val_loss: 3.0364e-04 - val_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.9289e-04 - val_accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 9.4060e-04 - accuracy: 1.0000 - val_loss: 2.8329e-04 - val_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 8.8850e-04 - accuracy: 1.0000 - val_loss: 2.7380e-04 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cecd54d2b0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model - TRANSFER LEARNING\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
    "    model.add(Bidirectional(GRU(lstm_unit, return_sequences=True)))\n",
    "    model.add(Bidirectional(GRU(lstm_unit // 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# # Train model for domain 1\n",
    "print(\"Domain 1 model:\")\n",
    "model1 = create_model()\n",
    "model1.fit(X_train1, y_train1, epochs=num_epoch, batch_size=num_batch, validation_data=(X_val1, y_val1), \n",
    "           verbose=1, \n",
    "#            class_weight=class_weights_dict, \n",
    "           callbacks=[model_checkpoint_callback1, early_stopping])\n",
    "\n",
    "print(\"-\"*40)\n",
    "print(\"Domain 2 model:\")\n",
    "num_folds = 3\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "best_val_loss = float('inf')\n",
    "fold_accuracies = []\n",
    "\n",
    "# Prepare the data for domain 2\n",
    "X_train2 = pad_sequences(dataset2['txt'], maxlen=max_len)\n",
    "y_train2 = dataset2[\"label\"].values\n",
    "\n",
    "# Transfer learning\n",
    "model2 = load_model(checkpoint_filepath1)\n",
    "model2.pop()\n",
    "\n",
    "for layer in model2.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2.fit(X_train2, y_train2, epochs=num_epoch, batch_size=num_batch, validation_data=(X_val2, y_val2), \n",
    "           verbose=1, \n",
    "#            class_weight=class_weights_dict, \n",
    "           callbacks=[model_checkpoint_callback2, early_stopping])\n",
    "\n",
    "# for i, (train_index, val_index) in enumerate(kf.split(X_train2)):\n",
    "#     print(f\"\\nFold {i + 1}/{num_folds}\")\n",
    "#     X_train_fold, X_val_fold = X_train2[train_index], X_train2[val_index]\n",
    "#     y_train_fold, y_val_fold = y_train2[train_index], y_train2[val_index]\n",
    "\n",
    "#     model2.fit(X_train_fold, y_train_fold, epochs=num_epoch, batch_size=num_batch,\n",
    "#                validation_data=(X_val_fold, y_val_fold), verbose=1,\n",
    "#                callbacks=[model_checkpoint_callback2, early_stopping])\n",
    "#     model2 = load_model(checkpoint_filepath2)\n",
    "#     val_loss, _ = model2.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         best_model2 = model2\n",
    "#         print(\"best model updated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "11d33996",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "40/40 [==============================] - 2s 29ms/step\n",
      "16/16 [==============================] - 1s 22ms/step\n",
      "\n",
      "Validation\n",
      "Domain1 Validation Accuracy: 0.89140625\n",
      "Domain2 Validation Accuracy: 1.0\n",
      "----------------------------------------\n",
      "Mock Test\n",
      "19/19 [==============================] - 1s 21ms/step\n",
      "13/13 [==============================] - 0s 24ms/step\n",
      "Mock Test1 Accuracy: 0.885\n",
      "Mock Test2 Accuracy: 1.0\n",
      "Combined Mock Test Accuracy: 0.931\n"
     ]
    }
   ],
   "source": [
    "# Load the best models\n",
    "best_model1 = load_model(checkpoint_filepath1)\n",
    "best_model2 = load_model(checkpoint_filepath2)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "y_pred_val1 = (best_model1.predict(X_val1) > 0.5).astype(\"int32\")\n",
    "y_pred_val2 = (best_model2.predict(X_train2) > 0.5).astype(\"int32\")\n",
    "print(\"\\nValidation\")\n",
    "print(\"Domain1 Validation Accuracy:\", accuracy_score(y_val1, y_pred_val1))\n",
    "print(\"Domain2 Validation Accuracy:\", accuracy_score(y_train2, y_pred_val2))\n",
    "\n",
    "# mock test\n",
    "print(\"-\"*40)\n",
    "print(\"Mock Test\")\n",
    "# Split the combined mock test set into two parts: first 600 for model1 and next 400 for model2\n",
    "X_mock_test1 = X_combined_mock_test[:600]\n",
    "y_mock_test1 = y_combined_mock_test[:600]\n",
    "\n",
    "X_mock_test2 = X_combined_mock_test[600:]\n",
    "y_mock_test2 = y_combined_mock_test[600:]\n",
    "\n",
    "# Make predictions using the respective models\n",
    "y_pred_mock_test1 = (best_model1.predict(X_mock_test1) > 0.5).astype(\"int32\")\n",
    "y_pred_mock_test2 = (best_model2.predict(X_mock_test2) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Calculate the accuracy for each part\n",
    "accuracy_mock_test1 = accuracy_score(y_mock_test1, y_pred_mock_test1)\n",
    "accuracy_mock_test2 = accuracy_score(y_mock_test2, y_pred_mock_test2)\n",
    "print(\"Mock Test1 Accuracy:\", accuracy_mock_test1)\n",
    "print(\"Mock Test2 Accuracy:\", accuracy_mock_test2)\n",
    "\n",
    "# Combine the predictions and true labels\n",
    "y_pred_combined_mock_test = np.concatenate([y_pred_mock_test1, y_pred_mock_test2])\n",
    "y_true_combined_mock_test = np.concatenate([y_mock_test1, y_mock_test2])\n",
    "\n",
    "# Calculate the overall accuracy\n",
    "accuracy_combined_mock_test = accuracy_score(y_true_combined_mock_test, y_pred_combined_mock_test)\n",
    "print(\"Combined Mock Test Accuracy:\", accuracy_combined_mock_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f04ee7a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 25ms/step\n",
      "13/13 [==============================] - 0s 25ms/step\n",
      "Done csv output.\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "test_df = json_to_dataset(\"./data/test.json\")\n",
    "\n",
    "# Pad test sequences\n",
    "X_test = pad_sequences(test_df['txt'], maxlen=max_len)\n",
    "\n",
    "# Make predictions\n",
    "X_test_domain1 = X_test[:600]\n",
    "X_test_domain2 = X_test[600:]\n",
    "\n",
    "predictions_domain1 = best_model1.predict(X_test_domain1)\n",
    "predictions_domain2 = best_model2.predict(X_test_domain2)\n",
    "\n",
    "# Combine predictions\n",
    "predictions = np.concatenate([predictions_domain1, predictions_domain2])\n",
    "\n",
    "# Convert predictions to binary labels\n",
    "binary_predictions = (predictions > 0.5).astype(int).flatten()\n",
    "\n",
    "# Create a submission DataFrame\n",
    "submission_df = pd.DataFrame({\"Id\": np.arange(len(binary_predictions)), \"Predicted\": binary_predictions})\n",
    "\n",
    "# Save the submission DataFrame as a CSV file\n",
    "submission_df.to_csv(\"bidirection_transfer_cv.csv\", index=False)\n",
    "print(\"Done csv output.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e3c8e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
